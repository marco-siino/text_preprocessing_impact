{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/text_preprocessing_impact/blob/main/20N_DS/SVM_20N_TextPreProImpact_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\n",
        "- - -\n",
        "SVM ON 20NewsGroup DS EXPERIMENTS NOTEBOOK\n",
        "- - -\n",
        "Support Vector Machine on 20Newsgroup Dataset.\n",
        "Code by M. Siino.\n",
        "\n",
        "From the paper: \"Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQSunQ-ucjLX",
        "outputId": "020bdec8-08bb-4315-b9f2-417bc5d9d425"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from google.colab import files\n",
        "from io import open\n",
        "from numpy.random import seed\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn import svm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']='true'\n",
        "os.environ['TF_DETERMINISTIC_OPS']='true'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYMUXaY8r0_",
        "outputId": "6c80ebea-7b47-417f-9dc6-9a72dca2308e"
      },
      "source": [
        "urlDataset = \"https://github.com/marco-siino/text_preprocessing_impact/raw/main/20N_DS/20news-bydate.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"20news-bydate.tar.gz\", urlDataset,\n",
        "                                    extract=True, archive_format='tar',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "test_dir = '20news-bydate-test'\n",
        "train_dir = '20news-bydate-train'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/marco-siino/text_preprocessing_impact/raw/main/20N_DS/20news-bydate.tar.gz\n",
            "14464277/14464277 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7jOZjALo4X"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ucATWhfGSGf"
      },
      "source": [
        "def walk_dir(targetdir,topdown=True):\n",
        "    for root, dirs, files in os.walk(targetdir, topdown):\n",
        "        for subfolder in dirs:\n",
        "          #print(subfolder)\n",
        "          #print(root)\n",
        "          #print(dirs)\n",
        "          #print(files)\n",
        "          for subroot, subdirs, files in os.walk(root+'/'+subfolder, topdown):\n",
        "            #print(files)\n",
        "            for name in files:\n",
        "              #print(name)\n",
        "              os.rename(root+'/'+subfolder+'/'+name, root+'/'+subfolder+'/'+name+'.txt')\n",
        "\n",
        "walk_dir(test_dir)\n",
        "walk_dir(train_dir)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJi1fbT_Rup"
      },
      "source": [
        "## Generate full training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyafvXEMhEKw",
        "outputId": "19dbc648-ce1d-41c9-a9e7-117ef60950ab"
      },
      "source": [
        "# Generate full randomized training set.\n",
        "batch_size=1\n",
        "\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "train_ds=train_ds.shuffle(11314,seed=1, reshuffle_each_iteration=False)\n",
        "test_ds=test_ds.shuffle(7532,seed=1, reshuffle_each_iteration=False)\n",
        "\n",
        "train_ds_size=len(train_ds)\n",
        "test_ds_size=len(test_ds)\n",
        "\n",
        "train_ds = train_ds.take(200)\n",
        "test_ds = test_ds.take(50)\n",
        "\n",
        "\n",
        "#train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, depth=20)))\n",
        "#test_ds = test_ds.map(lambda x, y: (x, tf.one_hot(y, depth=20)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11314 files belonging to 20 classes.\n",
            "Found 7532 files belonging to 20 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Functions to pre-process source text. (A detailed discussion on our paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "source": [
        "# Do-Nothing preprocessing function.\n",
        "def DON(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data\n",
        "\n",
        "# Lowercasing preprocessing function.\n",
        "def LOW(input_data):\n",
        "  return tf.strings.lower(DON(input_data))\n",
        "\n",
        "# Removing Stop Words function.\n",
        "def RSW(input_data):\n",
        "  output_data = DON(input_data)\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.\n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "\n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.\n",
        "  else:\n",
        "\n",
        "    try:\n",
        "\n",
        "      # input_string = input_string.numpy() [0]\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"Tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data\n",
        "\n",
        "# Porter Stemmer preprocessing function.\n",
        "def STM(input_data):\n",
        "  output_data = DON(input_data)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.\n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "\n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      #print(input_string)\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.\n",
        "  else:\n",
        "\n",
        "    try:\n",
        "      #input_string = input_string.numpy()[0]\n",
        "      input_string = input_string.numpy()\n",
        "      #print(input_string)\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgfUp_5TWED7"
      },
      "source": [
        "## Define the combined preprocessing functions. (The base functions are: DON, LOW, RSW and STM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtlue6FuWIaB"
      },
      "source": [
        "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
        "#...5\n",
        "def LOW_RSW(input_data):\n",
        "  return RSW(LOW(input_data))\n",
        "\n",
        "# 6\n",
        "def LOW_STM(input_data):\n",
        "  return STM(LOW(input_data))\n",
        "\n",
        "# 7\n",
        "def RSW_LOW(input_data):\n",
        "  return LOW(RSW(input_data))\n",
        "\n",
        "# 8\n",
        "def RSW_STM(input_data):\n",
        "  return STM(RSW(input_data))\n",
        "\n",
        "# 9\n",
        "def STM_LOW(input_data):\n",
        "  return LOW(STM(input_data))\n",
        "\n",
        "# 10\n",
        "def STM_RSW(input_data):\n",
        "  return RSW(STM(input_data))\n",
        "\n",
        "# 11\n",
        "def LOW_STM_RSW(input_data):\n",
        "  return RSW(STM(LOW(input_data)))\n",
        "\n",
        "# 12\n",
        "def LOW_RSW_STM(input_data):\n",
        "  return STM(RSW(LOW(input_data)))\n",
        "\n",
        "# 13\n",
        "def STM_LOW_RSW(input_data):\n",
        "  return RSW(LOW(STM(input_data)))\n",
        "\n",
        "# 14\n",
        "def STM_RSW_LOW(input_data):\n",
        "  return LOW(RSW(STM(input_data)))\n",
        "\n",
        "# 15\n",
        "def RSW_LOW_STM(input_data):\n",
        "  return STM(LOW(RSW(input_data)))\n",
        "\n",
        "# 16\n",
        "def RSW_STM_LOW(input_data):\n",
        "  return LOW(STM(RSW(input_data)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjUnE9_DEyCw"
      },
      "source": [
        "## Get the length of the longest sample in training set. Then adapt text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebRMQqa_mr48"
      },
      "source": [
        "max_features = 0\n",
        "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
        "  # Set a large sequence length to find the longest sample in the training set.\n",
        "  sequence_length = 15000\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length,\n",
        "      encoding='ISO-8859-1')\n",
        "\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  #vectorize_layer.get_vocabulary()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  model.add(vectorize_layer)\n",
        "\n",
        "  longest_sample_length=1\n",
        "\n",
        "  for element in training_set:\n",
        "    authorDocument=element[0]\n",
        "    label=element[1]\n",
        "\n",
        "    #print(\"Sample considered is: \", authorDocument[0].numpy())\n",
        "    #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "    #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "    out=model(authorDocument)\n",
        "    # Convert token list to numpy array.\n",
        "    token_list = out.numpy()[0]\n",
        "    token_list = np.trim_zeros(token_list,'b')\n",
        "    if longest_sample_length < len(token_list):\n",
        "      longest_sample_length = len(token_list)\n",
        "\n",
        "  print(\"Length of the longest sample is:\", longest_sample_length)\n",
        "\n",
        "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
        "  sequence_length = longest_sample_length\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length,\n",
        "      encoding='ISO-8859-1')\n",
        "\n",
        "  # Finally adapt the vectorize layer.\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  global max_features\n",
        "  max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
        "  return vectorize_layer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8zDXrk9eEy5"
      },
      "source": [
        "## Define a dictionary with -> function_names:prepro_function_caller. And a dictionary to store model results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8owxNMIeFsX",
        "outputId": "0cbd4a8a-48b1-477f-db97-da41d296822a"
      },
      "source": [
        "model_results = {}\n",
        "prepro_functions_dict_base = {\n",
        "    'DON':DON,\n",
        "    'LOW':LOW,\n",
        "    'RSW':RSW,\n",
        "    'STM':STM\n",
        "    }\n",
        "\n",
        "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
        "\n",
        "prepro_functions_dict_comb = {\n",
        "    # 1. Do nothing\n",
        "    'DON': DON,\n",
        "    # 2. Lowercasing\n",
        "    'LOW':LOW,\n",
        "    # 3. Removing Stopwords\n",
        "    'RSW':RSW,\n",
        "    # 4. Porter Stemming\n",
        "    'STM':STM,\n",
        "    # 5. LOW->RSW\n",
        "    'LOW_RSW':LOW_RSW,\n",
        "    # 6. LOW->STM\n",
        "    'LOW_STM':LOW_STM,\n",
        "    # 7. RSW->LOW\n",
        "    'RSW_LOW':RSW_LOW,\n",
        "    # 8. RSW->STM\n",
        "    'RSW_STM':RSW_STM,\n",
        "    # 9. STM->LOW\n",
        "    'STM_LOW':STM_LOW,\n",
        "    # 10. STM->RSW\n",
        "    'STM_RSW':STM_RSW,\n",
        "    # 11. LOW->STM->RSW\n",
        "    'LOW_STM_RSW':LOW_STM_RSW,\n",
        "    # 12. LOW->RSW->STM\n",
        "    'LOW_RSW_STM':LOW_RSW_STM,\n",
        "    # 13. STM->LOW->RSW\n",
        "    'STM_LOW_RSW':STM_LOW_RSW,\n",
        "    # 14. STM->RSW->LOW\n",
        "    'STM_RSW_LOW':STM_RSW_LOW,\n",
        "    # 15. RSW->LOW->STM\n",
        "    'RSW_LOW_STM':RSW_LOW_STM,\n",
        "    # 16. RSW->STM->LOW\n",
        "    'RSW_STM_LOW':RSW_STM_LOW\n",
        "}\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  print(key)\n",
        "  model_results[key]=[]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON\n",
            "LOW\n",
            "RSW\n",
            "STM\n",
            "LOW_RSW\n",
            "LOW_STM\n",
            "RSW_LOW\n",
            "RSW_STM\n",
            "STM_LOW\n",
            "STM_RSW\n",
            "LOW_STM_RSW\n",
            "LOW_RSW_STM\n",
            "STM_LOW_RSW\n",
            "STM_RSW_LOW\n",
            "RSW_LOW_STM\n",
            "RSW_STM_LOW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDEi7MAo4qsQ"
      },
      "source": [
        "## Models definition and evaluation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1fdCpETWL-",
        "outputId": "f72f3f83-8fa9-43bb-9001-f348a2b222af"
      },
      "source": [
        "for key in prepro_functions_dict_comb:\n",
        "    print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
        "\n",
        "    # Preprocess training set to build a dictionary.\n",
        "    vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
        "\n",
        "    print(\"\\n\\n***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\")\n",
        "    # Print a raw and a preprocessed sample.\n",
        "    for element in train_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "\n",
        "      print(\"Sample considered is: \", authorDocument[0])\n",
        "      print(\"Preprocessed: \", str(prepro_functions_dict_comb[key](authorDocument[0].numpy())))\n",
        "      break\n",
        "\n",
        "    # # # - - - - - MODELS DEFINITION AND EVALUATION - - - - - # # #\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "    model.add(vectorize_layer)\n",
        "\n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "    # --- SVM SECTION START --- #\n",
        "\n",
        "    training_labels=[]\n",
        "    training_samples=[]\n",
        "\n",
        "    max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
        "\n",
        "    for element in train_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "\n",
        "      #print(\"Sample considered is: \", authorDocument[0])\n",
        "      #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "      #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      training_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        #print(current_token,end=' ')\n",
        "        #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "        current_sample[current_token]+=1\n",
        "      training_samples.append(current_sample)\n",
        "      #break\n",
        "\n",
        "    training_labels=np.array(training_labels)\n",
        "    training_samples=np.array(training_samples)\n",
        "    #print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "    #print(training_labels)\n",
        "    #print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "    #print(training_samples)\n",
        "\n",
        "    test_labels=[]\n",
        "    test_samples=[]\n",
        "\n",
        "    for element in test_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "\n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      test_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        current_sample[current_token]+=1\n",
        "      test_samples.append(current_sample)\n",
        "\n",
        "    test_labels=np.array(test_labels)\n",
        "    test_samples=np.array(test_samples)\n",
        "\n",
        "    SVM = svm.SVC(C=0.5, kernel='linear', gamma='auto')\n",
        "    SVM.fit(training_samples,training_labels)\n",
        "    # predict the labels on training set\n",
        "    #predictions_SVM = SVM.predict(training_samples)\n",
        "    # Use accuracy_score function to get the accuracy\n",
        "    #model_results[key]['SVM_train']=SVM.score(training_samples,training_labels)\n",
        "    #print(\"SVM Accuracy Score on Training set -> \",model_results[key]['SVM_train'])\n",
        "\n",
        "    # predict the labels on validation dataset\n",
        "    predictions_SVM = SVM.predict(test_samples)\n",
        "    # Use accuracy_score function to get the accuracy\n",
        "    model_results[key].append(SVM.score(test_samples,test_labels))\n",
        "    print(\"SVM Accuracy Score on Test set -> \",model_results[key])\n",
        "\n",
        "    # --- SVM SECTION END --- #\n",
        "\n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "    # # # - - - - - MODEL DEFINITION AND EVALUATION END  - - - - - # # #"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "* * * * EVALUATION USING DON AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 7969\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.16]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 7969\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from: wrat@unisql.uucp (wharfie)\\nsubject: re: too fast\\norganization: unisql, inc., austin, texas, usa\\nlines: 9\\n\\nin article <1993apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (john f nielsen) writes:\\n>there may be a case where a speed limit sign is not necessary. but take\\n>them away entirely?\\n\\n\\tyeah, you're right.  doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4871\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"From wrat unisql.UUCP wharfie nSubject Re Too fast\\\\nOrganization UniSQL Inc Austin Texas USA\\\\nLines 9\\\\n\\\\nIn article 1993Apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu John F Nielsen writes n There may case speed limit sign necessary But take\\\\n away entirely n\\\\n\\\\tYeah 're right Doing away speed limits would just\\\\nmean huge tax increases municipalities tried make the\\\\nrevenue used gouge passing motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.1]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 8327\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\norgan unisql inc austin texa usa\\\\nlin 9\\\\n\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may be a case where a speed limit sign is not necessari but take\\\\n them away entir n\\\\n\\\\tyeah you 're right do away with speed limit would just\\\\nmean huge tax increas as municip tri to make up for the\\\\nrevenu they use to goug from pass motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.16]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4472\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfie nsubject fast\\\\norganization unisql inc austin texas usa\\\\nlines 9\\\\n\\\\nin article 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen writes n may case speed limit sign necessary take\\\\n away entirely n\\\\n\\\\tyeah 're right away speed limits would just\\\\nmean huge tax increases municipalities tried make the\\\\nrevenue used gouge passing motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 8327\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\norgan unisql inc austin texa usa\\\\nlin 9\\\\n\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may be a case where a speed limit sign is not necessari but take\\\\n them away entir n\\\\n\\\\tyeah you 're right do away with speed limit would just\\\\nmean huge tax increas as municip tri to make up for the\\\\nrevenu they use to goug from pass motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.12]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4871\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfie nsubject re too fast\\\\norganization unisql inc austin texas usa\\\\nlines 9\\\\n\\\\nin article 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen writes n there may case speed limit sign necessary but take\\\\n away entirely n\\\\n\\\\tyeah 're right doing away speed limits would just\\\\nmean huge tax increases municipalities tried make the\\\\nrevenue used gouge passing motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.12]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4871\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may case speed limit sign necessari but take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right do away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.16]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 8327\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\norgan unisql inc austin texa usa\\\\nlin 9\\\\n\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may be a case where a speed limit sign is not necessari but take\\\\n them away entir n\\\\n\\\\tyeah you 're right do away with speed limit would just\\\\nmean huge tax increas as municip tri to make up for the\\\\nrevenu they use to goug from pass motorists.\\\\n\\\\n\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.12]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4727\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfi nsubject fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n may case speed limit sign necessari take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4727\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfi nsubject fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n may case speed limit sign necessari take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4472\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfi nsubject fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n may case speed limit sign necessari take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4727\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfi nsubject fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n may case speed limit sign necessari take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4727\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"wrat unisql.uucp wharfi nsubject fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n may case speed limit sign necessari take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4871\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may case speed limit sign necessari but take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right do away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.14]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 4871\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b\"From: wrat@unisql.UUCP (wharfie)\\nSubject: Re: Too fast\\nOrganization: UniSQL, Inc., Austin, Texas, USA\\nLines: 9\\n\\nIn article <1993Apr14.152328.15997@magnus.acs.ohio-state.edu> jnielsen@magnus.acs.ohio-state.edu (John F Nielsen) writes:\\n>There may be a case where a speed limit sign is not necessary. But take\\n>them away entirely?\\n\\n\\tYeah, you're right.  Doing away with speed limits would just\\nmean huge tax increases as municipalities tried to make up for the\\nrevenue they used to gouge from passing motorists.\\n\\n\\n\", shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b\"from wrat unisql.uucp wharfi nsubject re too fast\\\\\\\\norgan unisql inc austin texa usa\\\\\\\\nlin 9\\\\\\\\n\\\\\\\\nin articl 1993apr14.152328.15997 magnus.acs.ohio-state.edu jnielsen magnus.acs.ohio-state.edu john f nielsen write n there may case speed limit sign necessari but take\\\\\\\\n away entir n\\\\\\\\n\\\\\\\\tyeah 're right do away speed limit would just\\\\\\\\nmean huge tax increas municip tri make the\\\\\\\\nrevenu use goug pass motorists.\\\\\\\\n\\\\\\\\n\\\\\\\\n\", shape=(), dtype=string)\n",
            "SVM Accuracy Score on Test set ->  [0.16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uMKAeuegNk1"
      },
      "source": [
        "## Now show compact results in a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ79RMougNRA",
        "outputId": "9f5d6fb4-ffa5-49ea-8c88-acee15009a89"
      },
      "source": [
        "print(\" PREPRO FUNCTION    |  Test Accuracy   |\",end = '')\n",
        "\n",
        "print(\"\\n\")\n",
        "for prepro_func in prepro_functions_dict_comb:\n",
        "  #print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\n",
        "  result = format(round(model_results[prepro_func][0],4),'.4f')\n",
        "  print(f'{prepro_func:27}{ result :12}')\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PREPRO FUNCTION    |  Test Accuracy   |\n",
            "\n",
            "DON                        0.1600      \n",
            "\n",
            "\n",
            "LOW                        0.1400      \n",
            "\n",
            "\n",
            "RSW                        0.1000      \n",
            "\n",
            "\n",
            "STM                        0.1600      \n",
            "\n",
            "\n",
            "LOW_RSW                    0.1400      \n",
            "\n",
            "\n",
            "LOW_STM                    0.1200      \n",
            "\n",
            "\n",
            "RSW_LOW                    0.1200      \n",
            "\n",
            "\n",
            "RSW_STM                    0.1600      \n",
            "\n",
            "\n",
            "STM_LOW                    0.1200      \n",
            "\n",
            "\n",
            "STM_RSW                    0.1400      \n",
            "\n",
            "\n",
            "LOW_STM_RSW                0.1400      \n",
            "\n",
            "\n",
            "LOW_RSW_STM                0.1400      \n",
            "\n",
            "\n",
            "STM_LOW_RSW                0.1400      \n",
            "\n",
            "\n",
            "STM_RSW_LOW                0.1400      \n",
            "\n",
            "\n",
            "RSW_LOW_STM                0.1400      \n",
            "\n",
            "\n",
            "RSW_STM_LOW                0.1600      \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}