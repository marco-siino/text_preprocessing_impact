{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NB_PCL_TextPreProImpact_NB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2KIzgX9N1hYH",
        "LkFaMbVK2Jky"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/text_preprocessing_impact/blob/main/PCL_DS/NB_PCL_TextPreProImpact_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "# Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances. \n",
        "- - - \n",
        "NB ON PCL DS EXPERIMENTS NOTEBOOK \n",
        "- - -\n",
        "Naive Bayes on Patronizing and Condescending Language Dataset.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\" by M.Siino et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSunQ-ucjLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91557d1-82c7-4523-a38d-3384ab3de05a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from io import open\n",
        "from pathlib import Path\n",
        "from urllib import request\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']='false'\n",
        "os.environ['TF_DETERMINISTIC_OPS']='false'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch all the files needed."
      ],
      "metadata": {
        "id": "SOaiX3dD1cRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlDontPatronizeMe_PCL_tsv = \"https://drive.google.com/uc?export=download&id=1KAncWruZ4OkKlvEGnxLkNbR1sxAErG8_\"\n",
        "urlDontPatronizeMe_categories_tsv = \"https://drive.google.com/uc?export=download&id=1KMZJsskzKLbM-kgYiwXIF0h3Shj4F0mM\"\n",
        "urlTestSet_csv = \"https://drive.google.com/uc?export=download&id=161-_6MH16_UHtLTqt0nd09l68pP5MEbQ\"\n",
        "urlDevSet_csv = \"https://drive.google.com/uc?export=download&id=1KNuZ_h7NXTSwEz3_0XkaEd4DUAyxojAU\"\n",
        "urlTrainSet_csv = \"https://drive.google.com/uc?export=download&id=1KVRrMC9UVwtQE9QfcLv8b11P5t2BhA8I\"\n",
        "\n",
        "tmp = tf.keras.utils.get_file(\"dontpatronizeme_pcl.tsv\", urlDontPatronizeMe_PCL_tsv,\n",
        "                                    extract=False, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "tmp = tf.keras.utils.get_file(\"dontpatronizeme_categories.tsv\", urlDontPatronizeMe_categories_tsv,\n",
        "                                    extract=False, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "tmp = tf.keras.utils.get_file(\"task4_test.csv\", urlTestSet_csv,\n",
        "                                    extract=False, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "tmp = tf.keras.utils.get_file(\"dev_semeval_parids-labels.csv\", urlDevSet_csv,\n",
        "                                    extract=False, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "tmp = tf.keras.utils.get_file(\"train_semeval_parids-labels.csv\", urlTrainSet_csv,\n",
        "                                    extract=False, cache_dir='.',\n",
        "                                    cache_subdir='')\n"
      ],
      "metadata": {
        "id": "xGwlVkyC1dDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Don't Patronize Me! data manager module\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W2CqeH1U2ZGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L47YqeZJ2Z7M",
        "outputId": "bc5f9c54-81ae-480a-cc8f-1c7578f414b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and load dpm"
      ],
      "metadata": {
        "id": "x7bku3D08Y49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dont_patronize_me import DontPatronizeMe\n",
        "dpm = DontPatronizeMe('.', '.')\n",
        "dpm.load_task1()\n",
        "dpm.load_task2(return_one_hot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnE2_Y4A8Zkt",
        "outputId": "10a334d0-8485-466f-d42b-ba801fa940e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map of label to numerical label:\n",
            "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load paragraph IDs"
      ],
      "metadata": {
        "id": "Y_M_CAp98gF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trids = pd.read_csv('train_semeval_parids-labels.csv')\n",
        "teids = pd.read_csv('dev_semeval_parids-labels.csv')"
      ],
      "metadata": {
        "id": "SM6vkyP98hAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trids.head())\n",
        "print(len(trids))\n",
        "print(len(teids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8h4Buo58ytT",
        "outputId": "821cfab8-001c-448e-f13a-b4a59daa2655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   par_id                  label\n",
            "0    4341  [1, 0, 0, 1, 0, 0, 0]\n",
            "1    4136  [0, 1, 0, 0, 0, 0, 0]\n",
            "2   10352  [1, 0, 0, 0, 0, 1, 0]\n",
            "3    8279  [0, 0, 0, 1, 0, 0, 0]\n",
            "4    1164  [1, 0, 0, 1, 1, 1, 0]\n",
            "8375\n",
            "2094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trids.par_id = trids.par_id.astype(str)\n",
        "teids.par_id = teids.par_id.astype(str)"
      ],
      "metadata": {
        "id": "atjRM8-yySbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rebuild training set (Task 1)"
      ],
      "metadata": {
        "id": "mm11wbzAyXq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [] # will contain par_id, keyword, country, label and text\n",
        "for idx in range(len(trids)):  \n",
        "  parid = trids.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset to retrieve `text` and binary label\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  #keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  #country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      #'keyword':keyword,\n",
        "      #'country':country,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n",
        "  "
      ],
      "metadata": {
        "id": "CLB2_zS2yW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trdf1 = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "YO4TMqbhyj5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMSwb9spykjX",
        "outputId": "9f198eb6-56b0-4596-d902-ef9dd9959e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8375"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trdf1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBq3khibyq_D",
        "outputId": "ead66cf8-c9c6-4c3d-f1aa-4b57f072ad21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     par_id                                               text  label\n",
            "0      4341  The scheme saw an estimated 150,000 children f...      1\n",
            "1      4136  Durban 's homeless communities reconciliation ...      1\n",
            "2     10352  The next immediate problem that cropped up was...      1\n",
            "3      8279  Far more important than the implications for t...      1\n",
            "4      1164  To strengthen child-sensitive social protectio...      1\n",
            "...     ...                                                ...    ...\n",
            "8370   8380  Rescue teams search for survivors on the rubbl...      0\n",
            "8371   8381  The launch of ' Happy Birthday ' took place la...      0\n",
            "8372   8382  The unrest has left at least 20,000 people dea...      0\n",
            "8373   8383  You have to see it from my perspective . I may...      0\n",
            "8374   8384  Yet there was one occasion when we went to the...      0\n",
            "\n",
            "[8375 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next row to balance the DS.\n",
        "bal_trdf1 = trdf1.drop(trdf1.index[-6787:])\n",
        "\n",
        "print(\"Total number of samples is:\",len(bal_trdf1))\n",
        "print(\"Total number of class 1 samples is:\",sum(bal_trdf1.label == 1))\n",
        "print(\"Total number of class 0 samples is:\",sum(bal_trdf1.label == 0))\n",
        "\n",
        "trdf1 = bal_trdf1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC7FDHi9BPo0",
        "outputId": "285aa5cc-325b-4326-ea01-ae295c05f054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples is: 1588\n",
            "Total number of class 1 samples is: 794\n",
            "Total number of class 0 samples is: 794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rebuild test set (Task 1)"
      ],
      "metadata": {
        "id": "2m5BAKDryv_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [] # will contain par_id, label and text\n",
        "for idx in range(len(teids)):  \n",
        "  parid = teids.par_id[idx]\n",
        "  #print(parid)\n",
        "  # select row from original dataset\n",
        "  text = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].text.values[0]\n",
        "  label = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].label.values[0]\n",
        "  #keyword = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].keyword.values[0]\n",
        "  #country = dpm.train_task1_df.loc[dpm.train_task1_df.par_id == parid].country.values[0]\n",
        "  rows.append({\n",
        "      'par_id':parid,\n",
        "      #'keyword':keyword,\n",
        "      #'country':country,\n",
        "      'text':text,\n",
        "      'label':label\n",
        "  })\n",
        "   "
      ],
      "metadata": {
        "id": "9nnjlZTcyu78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgbNkRXhy-oW",
        "outputId": "6c3ba9a2-d098-4dfd-f842-9677292a34b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2094"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tedf1 = pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "F-oBVul3zA3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next row to balance the DS.\n",
        "bal_tedf1 = tedf1.drop(tedf1.index[-1696:])\n",
        "\n",
        "print(\"Total number of samples is:\",len(bal_tedf1))\n",
        "print(\"Total number of class 1 samples is:\",sum(bal_tedf1.label == 1))\n",
        "print(\"Total number of class 0 samples is:\",sum(bal_tedf1.label == 0))\n",
        "\n",
        "tedf1 = bal_tedf1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akbZRnzPCm3P",
        "outputId": "34cb374a-7e37-4bf5-a3c8-ff18337bb4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples is: 398\n",
            "Total number of class 1 samples is: 199\n",
            "Total number of class 0 samples is: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing functions definitions"
      ],
      "metadata": {
        "id": "GcOZCxIwzRH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do-Nothing preprocessing function.\n",
        "def DON(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data\n",
        "\n",
        "# Lowercasing preprocessing function.\n",
        "def LOW(input_data):  \n",
        "  return tf.strings.lower(DON(input_data))\n",
        "\n",
        "# Removing Stop Words function.\n",
        "def RSW(input_data):\n",
        "  output_data = DON(input_data)\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "\n",
        "      # input_string = input_string.numpy() [0]\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"Tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data\n",
        "\n",
        "# Porter Stemmer preprocessing function.\n",
        "def STM(input_data):\n",
        "  output_data = DON(input_data)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      #print(input_string)\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "      #input_string = input_string.numpy()[0]\n",
        "      input_string = input_string.numpy()\n",
        "      #print(input_string)\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data"
      ],
      "metadata": {
        "id": "okP2adtyzLxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the combined preprocessing functions. (The base functions are: DON, LOW, RSW and STM)."
      ],
      "metadata": {
        "id": "2KIzgX9N1hYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
        "#...5\n",
        "def LOW_RSW(input_data):\n",
        "  return RSW(LOW(input_data))\n",
        "\n",
        "# 6\n",
        "def LOW_STM(input_data):\n",
        "  return STM(LOW(input_data))\n",
        "\n",
        "# 7\n",
        "def RSW_LOW(input_data):\n",
        "  return LOW(RSW(input_data))\n",
        "\n",
        "# 8\n",
        "def RSW_STM(input_data):\n",
        "  return STM(RSW(input_data))\n",
        "\n",
        "# 9\n",
        "def STM_LOW(input_data):\n",
        "  return LOW(STM(input_data))\n",
        "\n",
        "# 10\n",
        "def STM_RSW(input_data):\n",
        "  return RSW(STM(input_data))\n",
        "  \n",
        "# 11\n",
        "def LOW_STM_RSW(input_data):\n",
        "  return RSW(STM(LOW(input_data)))\n",
        "\n",
        "# 12\n",
        "def LOW_RSW_STM(input_data):\n",
        "  return STM(RSW(LOW(input_data)))\n",
        "\n",
        "# 13\n",
        "def STM_LOW_RSW(input_data):\n",
        "  return RSW(LOW(STM(input_data)))\n",
        "\n",
        "# 14\n",
        "def STM_RSW_LOW(input_data):\n",
        "  return LOW(RSW(STM(input_data)))\n",
        "\n",
        "# 15\n",
        "def RSW_LOW_STM(input_data):\n",
        "  return STM(LOW(RSW(input_data)))\n",
        "\n",
        "# 16\n",
        "def RSW_STM_LOW(input_data):\n",
        "  return LOW(STM(RSW(input_data)))"
      ],
      "metadata": {
        "id": "z1WGe0fM1iaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Tensorflow DS"
      ],
      "metadata": {
        "id": "LkFaMbVK2Jky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "train_ds = False\n",
        "for i in range(0,len(trdf1)):\n",
        "  sample = [' '+trdf1['text'][i]]\n",
        "  label = [trdf1['label'][i]]\n",
        "\n",
        "  current_set = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            [tf.cast(sample,tf.string)],\n",
        "            [tf.cast(label, tf.int32)]\n",
        "        )\n",
        "    )\n",
        "  )\n",
        "  if train_ds != False:\n",
        "    train_ds = train_ds.concatenate(current_set)\n",
        "  else:\n",
        "    train_ds = current_set\n",
        "train_ds = train_ds.shuffle(len(train_ds), reshuffle_each_iteration=False)\n",
        "\n",
        "test_ds = False\n",
        "for i in range(0,len(tedf1)):\n",
        "  sample = [' '+tedf1['text'][i]]\n",
        "  label = [tedf1['label'][i]]\n",
        "\n",
        "  current_set = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            [tf.cast(sample,tf.string)],\n",
        "            [tf.cast(label, tf.int32)]\n",
        "        )\n",
        "    )\n",
        "  )\n",
        "  if test_ds != False:\n",
        "    test_ds = test_ds.concatenate(current_set)\n",
        "  else:\n",
        "    test_ds = current_set\n",
        "\n",
        "for element in train_ds:\n",
        "    authorDocument=element[0]\n",
        "    label=element[1]\n",
        "    print(authorDocument)\n",
        "    print(label)\n",
        "    break\n",
        "    \n",
        "print(train_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98qOEXUH2Dvm",
        "outputId": "708e12fc-6efb-4702-88b8-d5e04409ac06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .'], shape=(1,), dtype=string)\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "<ShuffleDataset element_spec=(TensorSpec(shape=(1,), dtype=tf.string, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the length of the longest sample in training set. Then adapt text.\n",
        "\n"
      ],
      "metadata": {
        "id": "lcSX8Kt33kIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
        "  # Set a very large sequence length to find the longest sample in the training set.\n",
        "  sequence_length = 20000\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  #vectorize_layer.get_vocabulary()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  model.add(vectorize_layer)\n",
        "\n",
        "  longest_sample_length=1\n",
        "\n",
        "  for element in training_set:\n",
        "    authorDocument=element[0]\n",
        "    label=element[1]\n",
        "    \n",
        "    #print(\"Sample considered is: \", authorDocument[0].numpy())\n",
        "    #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "    #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "    # Count the number of zeros from the last non-zero token to the end of the sample. \n",
        "    # Shortest tokenized sample has less zeros than others.\n",
        "    out=model(authorDocument)\n",
        "    token_nr_index=sequence_length-1\n",
        "    current_sample_zeros_counter=0\n",
        "    while out.numpy()[0][token_nr_index]==0:\n",
        "      token_nr_index-=1\n",
        "      current_sample_zeros_counter+=1\n",
        "\n",
        "    shortest_padding_length=sequence_length-longest_sample_length\n",
        "    if current_sample_zeros_counter<shortest_padding_length:\n",
        "      longest_sample_length=sequence_length-current_sample_zeros_counter\n",
        "\n",
        "  #print(out.numpy()[0][3229:3400])\n",
        "  #print(longest_sample_length)\n",
        "\n",
        "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
        "  sequence_length = longest_sample_length\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  # Finally adapt the vectorize layer.\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  return vectorize_layer"
      ],
      "metadata": {
        "id": "H_6PFQyo3kv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary with -> function_names:prepro_function_caller. And a dictionary to store model results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zDn8boxy4Kap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = {}\n",
        "prepro_functions_dict_base = {\n",
        "    'DON':DON,\n",
        "    'LOW':LOW,\n",
        "    'RSW':RSW,\n",
        "    'STM':STM\n",
        "    }\n",
        "\n",
        "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
        "\n",
        "prepro_functions_dict_comb = {\n",
        "    # 1. Do nothing \n",
        "    'DON': DON,\n",
        "    # 2. Lowercasing \n",
        "    'LOW':LOW,\n",
        "    # 3. Removing Stopwords\n",
        "    'RSW':RSW, \n",
        "    # 4. Porter Stemming\n",
        "    'STM':STM,\n",
        "    # 5. LOW->RSW\n",
        "    'LOW_RSW':LOW_RSW, \n",
        "    # 6. LOW->STM\n",
        "    'LOW_STM':LOW_STM,\n",
        "    # 7. RSW->LOW\n",
        "    'RSW_LOW':RSW_LOW,\n",
        "    # 8. RSW->STM\n",
        "    'RSW_STM':RSW_STM,\n",
        "    # 9. STM->LOW\n",
        "    'STM_LOW':STM_LOW,\n",
        "    # 10. STM->RSW\n",
        "    'STM_RSW':STM_RSW,\n",
        "    # 11. LOW->STM->RSW\n",
        "    'LOW_STM_RSW':LOW_STM_RSW,  \n",
        "    # 12. LOW->RSW->STM\n",
        "    'LOW_RSW_STM':LOW_RSW_STM,\n",
        "    # 13. STM->LOW->RSW\n",
        "    'STM_LOW_RSW':STM_LOW_RSW,\n",
        "    # 14. STM->RSW->LOW\n",
        "    'STM_RSW_LOW':STM_RSW_LOW,\n",
        "    # 15. RSW->LOW->STM\n",
        "    'RSW_LOW_STM':RSW_LOW_STM,\n",
        "    # 16. RSW->STM->LOW\n",
        "    'RSW_STM_LOW':RSW_STM_LOW\n",
        "}\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  print(key)\n",
        "  model_results[key]=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etrhk_B-4LIe",
        "outputId": "a2365fd0-21d5-45ca-9d7c-a4e687833d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON\n",
            "LOW\n",
            "RSW\n",
            "STM\n",
            "LOW_RSW\n",
            "LOW_STM\n",
            "RSW_LOW\n",
            "RSW_STM\n",
            "STM_LOW\n",
            "STM_RSW\n",
            "LOW_STM_RSW\n",
            "LOW_RSW_STM\n",
            "STM_LOW_RSW\n",
            "STM_RSW_LOW\n",
            "RSW_LOW_STM\n",
            "RSW_STM_LOW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some training hyperparameters..."
      ],
      "metadata": {
        "id": "J4yWVdG-4sCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 100\n",
        "\n",
        "num_runs = 5 \n",
        "# No need to go over the 20th epoch...Overfitting begins.\n",
        "num_epochs_per_run = 20\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()"
      ],
      "metadata": {
        "id": "zTrF0mv74s-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models definition and evaluation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7pAYxmSI4544"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in prepro_functions_dict_comb:\n",
        "    print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
        "\n",
        "    # Preprocess training set to build a dictionary.\n",
        "    vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
        "\n",
        "    print(\"\\n\\n***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\")\n",
        "    # Print a raw and a preprocessed sample.\n",
        "    for element in train_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      print(\"Sample considered is: \", authorDocument[0])\n",
        "      print(\"Preprocessed: \", str(prepro_functions_dict_comb[key](authorDocument[0].numpy())))\n",
        "      break\n",
        "    \n",
        "    # # # - - - - - MODELS DEFINITION AND EVALUATION - - - - - # # #\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "    model.add(vectorize_layer)\n",
        "    \n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "\n",
        "    training_labels=[]\n",
        "    training_samples=[]\n",
        "\n",
        "    max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
        "\n",
        "    for element in train_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      #print(\"Sample considered is: \", authorDocument[0])\n",
        "      #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "      #print(\"And has label: \", label[0].numpy())\n",
        "      \n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      training_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        #print(current_token,end=' ')\n",
        "        #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "        current_sample[current_token]+=1\n",
        "      training_samples.append(current_sample)\n",
        "      #break\n",
        "\n",
        "    training_labels=np.array(training_labels)\n",
        "    training_samples=np.array(training_samples)\n",
        "    #print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "    #print(training_labels)\n",
        "    #print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "    #print(training_samples)\n",
        "\n",
        "    test_labels=[]\n",
        "    test_samples=[]\n",
        "\n",
        "    for element in test_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      test_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        current_sample[current_token]+=1\n",
        "      test_samples.append(current_sample)\n",
        "\n",
        "    test_labels=np.array(test_labels)\n",
        "    test_samples=np.array(test_samples)\n",
        "\n",
        "    NB = MultinomialNB()\n",
        "    NB.fit(training_samples,training_labels)\n",
        "    \n",
        "    model_results[key].append(NB.score(test_samples,test_labels))\n",
        "    print(\"NB Accuracy Score on Test set -> \",model_results[key])\n",
        "\n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "    \n",
        "    # # # - - - - - MODEL DEFINITION AND EVALUATION END  - - - - - # # #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qEwrhD046ii",
        "outputId": "06cf0fe5-0422-45d3-f8d3-bb326bd1e531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "* * * * EVALUATION USING DON AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.7261306532663316]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b' lusaka zambia ( xinhua ) -- zambia ? s immigration department said wednesday that it had arrested at least 45 illegal immigrants in lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.7361809045226131]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'LUSAKA Zambia Xinhua Zambia Immigration Department said Wednesday arrested least 45 illegal immigrants Lusaka country capital', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.7185929648241206]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia s immigr depart said wednesday that it had arrest at least 45 illeg immigr in lusaka the countri s capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6834170854271356]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigration department said wednesday arrested least 45 illegal immigrants lusaka country capital', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.7060301507537688]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia s immigr depart said wednesday that it had arrest at least 45 illeg immigr in lusaka the countri s capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.678391959798995]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigration department said wednesday arrested least 45 illegal immigrants lusaka country capital', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.7211055276381909]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6708542713567839]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia s immigr depart said wednesday that it had arrest at least 45 illeg immigr in lusaka the countri s capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.678391959798995]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6809045226130653]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.678391959798995]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6683417085427136]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.678391959798995]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.678391959798995]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6733668341708543]\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING RSW_STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "\n",
            "\n",
            "***** FINITO DI PROCESSARE E ADATTARE IL TRAINING SET, INIZIA LA SIMULAZIONE *******\n",
            "Sample considered is:  tf.Tensor(b' LUSAKA Zambia ( Xinhua ) -- Zambia ? s Immigration Department said Wednesday that it had arrested at least 45 illegal immigrants in Lusaka , the country ? s capital .', shape=(), dtype=string)\n",
            "Preprocessed:  tf.Tensor(b'lusaka zambia xinhua zambia immigr depart said wednesday arrest least 45 illeg immigr lusaka countri capit', shape=(), dtype=string)\n",
            "NB Accuracy Score on Test set ->  [0.6733668341708543]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now show compact results in a table."
      ],
      "metadata": {
        "id": "GYNXRz2mZB9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" PREPRO FUNCTION    |  Test Accuracy   |\",end = '')\n",
        "\n",
        "print(\"\\n\")\n",
        "for prepro_func in prepro_functions_dict_comb:\n",
        "  #print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\n",
        "  result = model_results[prepro_func][0]\n",
        "  # result = format(round(model_results[prepro_func][0],4),'.4f')\n",
        "  print(f'{prepro_func:27}{ result :12}')\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "hPJRm9aKZD0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9d3eee-742f-4b7c-a271-eaa4d61d3b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PREPRO FUNCTION    |  Test Accuracy   |\n",
            "\n",
            "DON                        0.7261306532663316\n",
            "\n",
            "\n",
            "LOW                        0.7361809045226131\n",
            "\n",
            "\n",
            "RSW                        0.7185929648241206\n",
            "\n",
            "\n",
            "STM                        0.6834170854271356\n",
            "\n",
            "\n",
            "LOW_RSW                    0.7060301507537688\n",
            "\n",
            "\n",
            "LOW_STM                    0.678391959798995\n",
            "\n",
            "\n",
            "RSW_LOW                    0.7211055276381909\n",
            "\n",
            "\n",
            "RSW_STM                    0.6708542713567839\n",
            "\n",
            "\n",
            "STM_LOW                    0.678391959798995\n",
            "\n",
            "\n",
            "STM_RSW                    0.6809045226130653\n",
            "\n",
            "\n",
            "LOW_STM_RSW                0.678391959798995\n",
            "\n",
            "\n",
            "LOW_RSW_STM                0.6683417085427136\n",
            "\n",
            "\n",
            "STM_LOW_RSW                0.678391959798995\n",
            "\n",
            "\n",
            "STM_RSW_LOW                0.678391959798995\n",
            "\n",
            "\n",
            "RSW_LOW_STM                0.6733668341708543\n",
            "\n",
            "\n",
            "RSW_STM_LOW                0.6733668341708543\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}