{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ITouXXtQ8WzV",
        "ouQCPPGiU7BY",
        "3lUNTVML4VdI",
        "9k63B8ax7E_p"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/text_preprocessing_impact/blob/main/FNS_DS/RoBERTa_FNS_TextPreProImpact_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances. \n",
        "- - - \n",
        "RoBERTa ON FNS DS EXPERIMENTS NOTEBOOK \n",
        "- - -\n",
        "RoBERTa on Fake News Spreaders Dataset.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSunQ-ucjLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1fe4f1-9cca-425b-e922-9e42aa752487"
      },
      "source": [
        "!pip install simpletransformers\n",
        "!pip install tensorboardx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from io import open\n",
        "from pathlib import Path\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpletransformers in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (0.63.9)\n",
            "Requirement already satisfied: tokenizers in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (1.1.1)\n",
            "Requirement already satisfied: streamlit in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (1.12.2)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (4.64.0)\n",
            "Requirement already satisfied: regex in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (2022.7.25)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (0.1.97)\n",
            "Requirement already satisfied: seqeval in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (2.4.0)\n",
            "Requirement already satisfied: tensorboard in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\domenico\\appdata\\roaming\\python\\python39\\site-packages (from simpletransformers) (1.22.4)\n",
            "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (0.13.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (1.4.3)\n",
            "Requirement already satisfied: requests in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (2.28.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (1.9.0)\n",
            "Requirement already satisfied: transformers>=4.6.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from simpletransformers) (4.22.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (0.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
            "Requirement already satisfied: six>=1.13.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.19.4)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\n",
            "Requirement already satisfied: setuptools in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (63.4.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.0.9)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.9.8)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
            "Requirement already satisfied: pathtools in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.9.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from requests->simpletransformers) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from requests->simpletransformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from requests->simpletransformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from requests->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.6 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (0.3.5.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (0.18.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (3.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from datasets->simpletransformers) (2022.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from pandas->simpletransformers) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from scikit-learn->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: watchdog in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (2.1.9)\n",
            "Requirement already satisfied: cachetools>=4.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (5.2.0)\n",
            "Requirement already satisfied: pympler>=0.9 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (1.5)\n",
            "Requirement already satisfied: altair>=3.2.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (4.2.0)\n",
            "Requirement already satisfied: semver in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (2.13.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (9.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (12.5.1)\n",
            "Requirement already satisfied: tornado>=5.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (6.2)\n",
            "Requirement already satisfied: validators>=0.2 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (0.20.0)\n",
            "Requirement already satisfied: toml in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (4.2)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (4.12.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (0.8.0b3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from streamlit->simpletransformers) (4.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (2.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboard->simpletransformers) (1.48.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.9.0)\n",
            "Requirement already satisfied: toolz in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (22.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.12.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (2022.2)\n",
            "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: tensorboardx in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\domenico\\appdata\\roaming\\python\\python39\\site-packages (from tensorboardx) (1.22.4)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in c:\\users\\domenico\\.conda\\envs\\tf\\lib\\site-packages (from tensorboardx) (3.19.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Domenico\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocYMUXaY8r0_",
        "outputId": "dfcd407e-05ab-4fa0-9775-88a06617b097"
      },
      "source": [
        "# Url obtained starting from this: https://drive.google.com/file/d/19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP/ and forcing export=download.\n",
        "urlTrainingSet = \"https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-training-2020-02-23.zip\"\n",
        "urlTestSet=\"https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-test-2020-02-23.zip\"\n",
        "\n",
        "training_set = tf.keras.utils.get_file(\"pan20-author-profiling-training-2020-02-23.zip\", urlTrainingSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "test_set = tf.keras.utils.get_file(\"pan20-author-profiling-test-2020-02-23.zip\", urlTestSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "training_set_dir = os.path.join(os.path.dirname(training_set), 'pan20-author-profiling-training-2020-02-23')\n",
        "test_set_dir = os.path.join(os.path.dirname(test_set), 'pan20-author-profiling-test-2020-02-23')\n",
        "\n",
        "print(training_set)\n",
        "print(training_set_dir)\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\\pan20-author-profiling-training-2020-02-23.zip\n",
            ".\\pan20-author-profiling-training-2020-02-23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di7jOZjALo4X"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ucATWhfGSGf",
        "outputId": "d7deff91-cb64-4759-9edb-b812363730c6"
      },
      "source": [
        "### Training Folders. ###\n",
        "\n",
        "# First level directory.\n",
        "if not os.path.exists('train_dir_en'):\n",
        "    os.makedirs('train_dir_en')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('train_dir_en/0'):\n",
        "    os.makedirs('train_dir_en/0')\n",
        "if not os.path.exists('train_dir_en/1'):\n",
        "    os.makedirs('train_dir_en/1')\n",
        "\n",
        "# Make Py variables.\n",
        "train_dir='train_dir_'\n",
        "\n",
        "## Test Folders. ##\n",
        "# First level directory.\n",
        "if not os.path.exists('test_dir_en'):\n",
        "    os.makedirs('test_dir_en')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('test_dir_en/0'):\n",
        "    os.makedirs('test_dir_en/0')\n",
        "if not os.path.exists('test_dir_en/1'):\n",
        "    os.makedirs('test_dir_en/1')\n",
        "\n",
        "# Make Py variables.\n",
        "test_dir='test_dir_'\n",
        "\n",
        "!ls -A"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNib56hF_8an"
      },
      "source": [
        "## Set language and directory paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq0EgZuf_5tv"
      },
      "source": [
        "# Set en and es ground truth file path for train_dir. We haven't a ground truth file for the test set.\n",
        "language='en'\n",
        "\n",
        "truth_file_training_dir_en=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_en = truth_file_training_dir_en+'truth.txt'\n",
        "\n",
        "truth_file_test_dir=test_set_dir\n",
        "truth_file_test_path_en = truth_file_test_dir+'/'+language+'.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VQKsc4XOpD8"
      },
      "source": [
        "## Read truth.txt to organize training and test dataset folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxcJ92-Nkto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "0de23a6a-d884-48b0-8678-8dfde7cdb6ec"
      },
      "source": [
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_en+fNameXml):\n",
        "      os.rename(truth_file_training_dir_en+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_en, \"r\")\n",
        "# use readline() to read the first line \n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Now move the file to the right folder.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(truth_file_training_dir_en\u001b[38;5;241m+\u001b[39mfNameXml):\n\u001b[1;32m---> 18\u001b[0m   \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruth_file_training_dir_en\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfNameXml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./train_dir_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfNameTxt\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# use readline() to read next line\u001b[39;00m\n\u001b[0;32m     21\u001b[0m line \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\n",
            "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: '.\\\\pan20-author-profiling-training-2020-02-23/en/4kgofb9pvi5pyjxjzjrplrgqj99njn2c.xml' -> './train_dir_en/0/4kgofb9pvi5pyjxjzjrplrgqj99njn2c.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the dataset."
      ],
      "metadata": {
        "id": "MRMgBDgsCua6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate full randomized training set.\n",
        "batch_size=1\n",
        "\n",
        "en_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "en_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "train_ds=en_train_ds.shuffle(300,seed=1, reshuffle_each_iteration=False)\n",
        "test_ds=en_test_ds.shuffle(200,seed=1, reshuffle_each_iteration=False)\n",
        "\n",
        "train_ds_size=len(train_ds)\n",
        "test_ds_size=len(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEu8rdcQCuHy",
        "outputId": "8580b0e9-862f-434a-d8a2-c8dd279339e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Functions to pre-process source text. (A detailed discussion on our paper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "source": [
        "# Do-Nothing preprocessing function.\n",
        "def DON(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data\n",
        "\n",
        "# Lowercasing preprocessing function.\n",
        "def LOW(input_data):  \n",
        "  return tf.strings.lower(DON(input_data))\n",
        "\n",
        "# Removing Stop Words function.\n",
        "def RSW(input_data):\n",
        "  output_data = DON(input_data)\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "\n",
        "      # input_string = input_string.numpy() [0]\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"Tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data\n",
        "\n",
        "# Porter Stemmer preprocessing function.\n",
        "def STM(input_data):\n",
        "  output_data = DON(input_data)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      #print(input_string)\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "      #input_string = input_string.numpy()[0]\n",
        "      input_string = input_string.numpy()\n",
        "      #print(input_string)\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the combined preprocessing functions. (The base functions are: DON, LOW, RSW and STM)."
      ],
      "metadata": {
        "id": "ouQCPPGiU7BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
        "#...5\n",
        "def LOW_RSW(input_data):\n",
        "  return RSW(LOW(input_data))\n",
        "\n",
        "# 6\n",
        "def LOW_STM(input_data):\n",
        "  return STM(LOW(input_data))\n",
        "\n",
        "# 7\n",
        "def RSW_LOW(input_data):\n",
        "  return LOW(RSW(input_data))\n",
        "\n",
        "# 8\n",
        "def RSW_STM(input_data):\n",
        "  return STM(RSW(input_data))\n",
        "\n",
        "# 9\n",
        "def STM_LOW(input_data):\n",
        "  return LOW(STM(input_data))\n",
        "\n",
        "# 10\n",
        "def STM_RSW(input_data):\n",
        "  return RSW(STM(input_data))\n",
        "  \n",
        "# 11\n",
        "def LOW_STM_RSW(input_data):\n",
        "  return RSW(STM(LOW(input_data)))\n",
        "\n",
        "# 12\n",
        "def LOW_RSW_STM(input_data):\n",
        "  return STM(RSW(LOW(input_data)))\n",
        "\n",
        "# 13\n",
        "def STM_LOW_RSW(input_data):\n",
        "  return RSW(LOW(STM(input_data)))\n",
        "\n",
        "# 14\n",
        "def STM_RSW_LOW(input_data):\n",
        "  return LOW(RSW(STM(input_data)))\n",
        "\n",
        "# 15\n",
        "def RSW_LOW_STM(input_data):\n",
        "  return STM(LOW(RSW(input_data)))\n",
        "\n",
        "# 16\n",
        "def RSW_STM_LOW(input_data):\n",
        "  return LOW(STM(RSW(input_data)))"
      ],
      "metadata": {
        "id": "f1PYTsbaU8AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary with -> function_names:prepro_function_caller. And a dictionary to store model results."
      ],
      "metadata": {
        "id": "3lUNTVML4VdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = {}\n",
        "prepro_functions_dict_base = {\n",
        "    'DON':DON,\n",
        "    'LOW':LOW,\n",
        "    'RSW':RSW,\n",
        "    'STM':STM\n",
        "    }\n",
        "\n",
        "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
        "\n",
        "prepro_functions_dict_comb = {\n",
        "    # 1. Do nothing \n",
        "    'DON': DON,\n",
        "    # 2. Lowercasing \n",
        "    'LOW':LOW,\n",
        "    # 3. Removing Stopwords\n",
        "    'RSW':RSW, \n",
        "    # 4. Porter Stemming\n",
        "    'STM':STM,\n",
        "    # 5. LOW->RSW\n",
        "    'LOW_RSW':LOW_RSW, \n",
        "    # 6. LOW->STM\n",
        "    'LOW_STM':LOW_STM,\n",
        "    # 7. RSW->LOW\n",
        "    'RSW_LOW':RSW_LOW,\n",
        "    # 8. RSW->STM\n",
        "    'RSW_STM':RSW_STM,\n",
        "    # 9. STM->LOW\n",
        "    'STM_LOW':STM_LOW,\n",
        "    # 10. STM->RSW\n",
        "    'STM_RSW':STM_RSW,\n",
        "    # 11. LOW->STM->RSW\n",
        "    'LOW_STM_RSW':LOW_STM_RSW,  \n",
        "    # 12. LOW->RSW->STM\n",
        "    'LOW_RSW_STM':LOW_RSW_STM,\n",
        "    # 13. STM->LOW->RSW\n",
        "    'STM_LOW_RSW':STM_LOW_RSW,\n",
        "    # 14. STM->RSW->LOW\n",
        "    'STM_RSW_LOW':STM_RSW_LOW,\n",
        "    # 15. RSW->LOW->STM\n",
        "    'RSW_LOW_STM':RSW_LOW_STM,\n",
        "    # 16. RSW->STM->LOW\n",
        "    'RSW_STM_LOW':RSW_STM_LOW\n",
        "}\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  print(key)\n",
        "  model_results[key]=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq8Ah1Wt4YjU",
        "outputId": "725661d1-6132-4dfa-9d04-2fb9dc2283ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON\n",
            "LOW\n",
            "RSW\n",
            "STM\n",
            "LOW_RSW\n",
            "LOW_STM\n",
            "RSW_LOW\n",
            "RSW_STM\n",
            "STM_LOW\n",
            "STM_RSW\n",
            "LOW_STM_RSW\n",
            "LOW_RSW_STM\n",
            "STM_LOW_RSW\n",
            "STM_RSW_LOW\n",
            "RSW_LOW_STM\n",
            "RSW_STM_LOW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to convert DSs to Pandas Dataframe"
      ],
      "metadata": {
        "id": "xmWtRjh18LcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_convert_ds(preprocessing_function):\n",
        "  # Convert English dataset.\n",
        "  train_df = [] # will contain text and label\n",
        "  for element in train_ds:\n",
        "    authorDocument=element[0]\n",
        "    label=int(element[1].numpy())\n",
        "    #print(authorDocument[0])\n",
        "    text = preprocessing_function(authorDocument[0].numpy()).numpy().decode('UTF-8')\n",
        "    train_df.append({\n",
        "        'text':text,\n",
        "        'label':label\n",
        "    })\n",
        "  train_df = pd.DataFrame(train_df)\n",
        "\n",
        "  test_df = [] # will contain text and label\n",
        "  for element in test_ds:\n",
        "    authorDocument=element[0]\n",
        "    label=int(element[1].numpy())\n",
        "    #print(authorDocument[0])\n",
        "    text = preprocessing_function(authorDocument[0].numpy()).numpy().decode('UTF-8')\n",
        "    test_df.append({\n",
        "        'text':text,\n",
        "        'label':label\n",
        "    })\n",
        "  test_df = pd.DataFrame(test_df)\n",
        "\n",
        "  return train_df, test_df\n"
      ],
      "metadata": {
        "id": "lV54jyB98P2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print some RAW and preprocessed samples (No need to execute)"
      ],
      "metadata": {
        "id": "9k63B8ax7E_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, element in enumerate(raw_train_ds_es):\n",
        "  if idx>1: break\n",
        "  authorDocument=element[0]\n",
        "  label=element[1]\n",
        "  temp = custom_standardization(authorDocument[0].numpy()).numpy().decode('UTF-8')\n",
        "  print(\"Not-Preprocessed samples: \\n\",authorDocument)\n",
        "  print(\"Preprocessed samples: \\n\",temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "o8MzLYcpgLQZ",
        "outputId": "e033a9c3-37a7-43a5-b718-0447277fff6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mraw_train_ds_es\u001b[49m):\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      3\u001b[0m   authorDocument\u001b[38;5;241m=\u001b[39melement[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'raw_train_ds_es' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some parameters definition..."
      ],
      "metadata": {
        "id": "VgvX5p5QB24X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check gpu\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "print('Cuda available? ',cuda_available)\n",
        "\n",
        "num_epochs_per_run = 10\n",
        "num_runs = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3YXCXSM7Nyv",
        "outputId": "af6e834a-d7ee-46e1-91b3-5cf1f779639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available?  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation of the model"
      ],
      "metadata": {
        "id": "hVY7v48weKX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in prepro_functions_dict_comb:\n",
        "  model_results[key]=[]\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "\n",
        "  model_args = ClassificationArgs(num_train_epochs=1, \n",
        "                                      no_save=True, \n",
        "                                      no_cache=True, \n",
        "                                      overwrite_output_dir=True)\n",
        "\n",
        "  model = ClassificationModel(\"roberta\", \n",
        "                                  'roberta-base', \n",
        "                                  args = model_args, \n",
        "                                  num_labels=2, \n",
        "                                  use_cuda=cuda_available)\n",
        "\n",
        "  runs_accuracy = []\n",
        "\n",
        "  print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
        "\n",
        "  # Preprocess train and test set and convert to DFs.\n",
        "  train_df,test_df = preprocess_and_convert_ds(prepro_functions_dict_comb[key])\n",
        "  \n",
        "  for run in range(1,(num_runs+1)):\n",
        "    epochs_accuracy=[]\n",
        "    model = ClassificationModel(\"roberta\", \n",
        "                                    'roberta-base', \n",
        "                                    args = model_args, \n",
        "                                    num_labels=2, \n",
        "                                    use_cuda=cuda_available)\n",
        "    for epoch in range (0,num_epochs_per_run):\n",
        "      print(\"\\nEPOCH NUMBER: \", epoch)\n",
        "      # train model\n",
        "      print(\"\\nNOW TRAIN THE MODEL.\")\n",
        "      model.train_model(train_df,show_running_loss=False)\n",
        "      print(\"\\nNOW EVALUATE THE TEST DF.\")\n",
        "      result, model_outputs, wrong_predictions = model.eval_model(test_df)\n",
        "      # Results on test set.\n",
        "      print(result)\n",
        "      correct_predictions = result['tp']+result['tn']\n",
        "      print(\"Correct predictions are: \",correct_predictions)\n",
        "      total_predictions = result['tp']+result['tn']+result['fp']+result['fn']\n",
        "      print(\"Total predictions are: \",total_predictions)\n",
        "      accuracy = correct_predictions/total_predictions\n",
        "      print(\"Accuracy on test set is:\",accuracy,\"\\n\\n\")\n",
        "      epochs_accuracy.append(accuracy)\n",
        "\n",
        "    print(epochs_accuracy)\n",
        "    runs_accuracy.append(max(epochs_accuracy))\n",
        "\n",
        "  runs_accuracy.sort()\n",
        "  print(\"\\n\\n Over all runs maximum accuracies are:\", runs_accuracy)\n",
        "  print(\"The median is:\",runs_accuracy[2])\n",
        "\n",
        "  if (runs_accuracy[2]-runs_accuracy[0])>(runs_accuracy[4]-runs_accuracy[2]):\n",
        "    max_range_from_median = runs_accuracy[2]-runs_accuracy[0]\n",
        "  else:\n",
        "    max_range_from_median = runs_accuracy[4]-runs_accuracy[2]\n",
        "  final_result = str(runs_accuracy[2])+\" +/- \"+ str(max_range_from_median)\n",
        "  model_results[key].append(final_result)\n",
        "  print(\"RoBERTa Accuracy Score on Test set -> \",model_results[key])\n"
      ],
      "metadata": {
        "id": "45Ip8xOYeLJy",
        "outputId": "6f614236-c988-4935-a066-f7324668577a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now show compact results in a table."
      ],
      "metadata": {
        "id": "VzTjFDal2jVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" PREPRO FUNCTION    |  Test Accuracy   |\",end = '')\n",
        "\n",
        "print(\"\\n\")\n",
        "for prepro_func in prepro_functions_dict_comb:\n",
        "  #print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\n",
        "  result = model_results[prepro_func][0]\n",
        "  # result = format(round(model_results[prepro_func][0],4),'.4f')\n",
        "  print(f'{prepro_func:27}{ result :12}')\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "-hyR5v7G2YW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03555a40-111c-4eff-acde-83563492bcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PREPRO FUNCTION    |  Test Accuracy   |\n",
            "\n",
            "DON                        0.695 +/- 0.019999999999999907\n",
            "\n",
            "\n",
            "LOW                        0.655 +/- 0.04500000000000004\n",
            "\n",
            "\n",
            "RSW                        0.705 +/- 0.015000000000000013\n",
            "\n",
            "\n",
            "STM                        0.66 +/- 0.030000000000000027\n",
            "\n",
            "\n",
            "LOW_RSW                    0.665 +/- 0.020000000000000018\n",
            "\n",
            "\n",
            "LOW_STM                    0.625 +/- 0.040000000000000036\n",
            "\n",
            "\n",
            "RSW_LOW                    0.67 +/- 0.020000000000000018\n",
            "\n",
            "\n",
            "RSW_STM                    0.65 +/- 0.15000000000000002\n",
            "\n",
            "\n",
            "STM_LOW                    0.66 +/- 0.135\n",
            "\n",
            "\n",
            "STM_RSW                    0.66 +/- 0.15500000000000003\n",
            "\n",
            "\n",
            "LOW_STM_RSW                0.64 +/- 0.10499999999999998\n",
            "\n",
            "\n",
            "LOW_RSW_STM                0.645 +/- 0.010000000000000009\n",
            "\n",
            "\n",
            "STM_LOW_RSW                0.64 +/- 0.14\n",
            "\n",
            "\n",
            "STM_RSW_LOW                0.64 +/- 0.14\n",
            "\n",
            "\n",
            "RSW_LOW_STM                0.645 +/- 0.125\n",
            "\n",
            "\n",
            "RSW_STM_LOW                0.64 +/- 0.015000000000000013\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}