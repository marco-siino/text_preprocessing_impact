{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/text_preprocessing_impact/blob/main/IMDB_DS/BiLSTM_IMDB_TextPreProImpact_NB_PART_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances. \n",
        "- - - \n",
        "BiLSTM ON IMDB DS EXPERIMENTS NOTEBOOK \n",
        "- - -\n",
        "Bidirectional Long Short-Term Memory Network on Internet Movie Database Dataset.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQSunQ-ucjLX",
        "outputId": "642d15c3-dfff-42fe-9ae7-b8aed3bba63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Domenico\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']='false'\n",
        "os.environ['TF_DETERMINISTIC_OPS']='false'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocYMUXaY8r0_"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_set_dir = os.path.join(dataset_dir, 'train')\n",
        "test_set_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "remove_dir = os.path.join(train_set_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJi1fbT_Rup"
      },
      "source": [
        "## Generate training and test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyafvXEMhEKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c55844ca-c676-4192-c4a0-bbe868283135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Generate full randomized training set.\n",
        "batch_size = 1\n",
        "seed = 1\n",
        "\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        "    )\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.shuffle(25000,seed=1,reshuffle_each_iteration = False)\n",
        "test_ds = test_ds.shuffle(25000,seed=1,reshuffle_each_iteration = False)\n",
        "\n",
        "train_ds = train_ds.take(5000)\n",
        "test_ds = test_ds.take(5000)\n",
        "\n",
        "train_ds_size=len(train_ds)\n",
        "test_ds_size=len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Functions to pre-process source text. (A detailed discussion on our paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "outputs": [],
      "source": [
        "# Do-Nothing preprocessing function.\n",
        "def DON(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data\n",
        "\n",
        "# Lowercasing preprocessing function.\n",
        "def LOW(input_data):  \n",
        "  return tf.strings.lower(DON(input_data))\n",
        "\n",
        "# Removing Stop Words function.\n",
        "def RSW(input_data):\n",
        "  output_data = DON(input_data)\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "\n",
        "      # input_string = input_string.numpy() [0]\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"Tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data\n",
        "\n",
        "# Porter Stemmer preprocessing function.\n",
        "def STM(input_data):\n",
        "  output_data = DON(input_data)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.  \n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "    \n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      #print(input_string)\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)  \n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.   \n",
        "  else:\n",
        "    \n",
        "    try:\n",
        "      #input_string = input_string.numpy()[0]\n",
        "      input_string = input_string.numpy()\n",
        "      #print(input_string)\n",
        "    \n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgfUp_5TWED7"
      },
      "source": [
        "## Define the combined preprocessing functions. (The base functions are: DON, LOW, RSW and STM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtlue6FuWIaB"
      },
      "outputs": [],
      "source": [
        "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
        "#...5\n",
        "def LOW_RSW(input_data):\n",
        "  return RSW(LOW(input_data))\n",
        "\n",
        "# 6\n",
        "def LOW_STM(input_data):\n",
        "  return STM(LOW(input_data))\n",
        "\n",
        "# 7\n",
        "def RSW_LOW(input_data):\n",
        "  return LOW(RSW(input_data))\n",
        "\n",
        "# 8\n",
        "def RSW_STM(input_data):\n",
        "  return STM(RSW(input_data))\n",
        "\n",
        "# 9\n",
        "def STM_LOW(input_data):\n",
        "  return LOW(STM(input_data))\n",
        "\n",
        "# 10\n",
        "def STM_RSW(input_data):\n",
        "  return RSW(STM(input_data))\n",
        "  \n",
        "# 11\n",
        "def LOW_STM_RSW(input_data):\n",
        "  return RSW(STM(LOW(input_data)))\n",
        "\n",
        "# 12\n",
        "def LOW_RSW_STM(input_data):\n",
        "  return STM(RSW(LOW(input_data)))\n",
        "\n",
        "# 13\n",
        "def STM_LOW_RSW(input_data):\n",
        "  return RSW(LOW(STM(input_data)))\n",
        "\n",
        "# 14\n",
        "def STM_RSW_LOW(input_data):\n",
        "  return LOW(RSW(STM(input_data)))\n",
        "\n",
        "# 15\n",
        "def RSW_LOW_STM(input_data):\n",
        "  return STM(LOW(RSW(input_data)))\n",
        "\n",
        "# 16\n",
        "def RSW_STM_LOW(input_data):\n",
        "  return LOW(STM(RSW(input_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjUnE9_DEyCw"
      },
      "source": [
        "## Get the length of the longest sample in training set. Then adapt text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebRMQqa_mr48"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
        "  # Set a large sequence length to find the longest sample in the training set.\n",
        "  sequence_length = 3000\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  #vectorize_layer.get_vocabulary()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  model.add(vectorize_layer)\n",
        "\n",
        "  longest_sample_length=1\n",
        "\n",
        "  for element in training_set:\n",
        "    authorDocument=element[0]\n",
        "    label=element[1]\n",
        "    \n",
        "    #print(\"Sample considered is: \", authorDocument[0].numpy())\n",
        "    #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "    #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "    out=model(authorDocument)\n",
        "    # Convert token list to numpy array.\n",
        "    token_list = out.numpy()[0]\n",
        "    token_list = np.trim_zeros(token_list,'b')\n",
        "    if longest_sample_length < len(token_list):\n",
        "      longest_sample_length = len(token_list)\n",
        "\n",
        "  print(\"Length of the longest sample is:\", longest_sample_length)\n",
        "\n",
        "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
        "  sequence_length = longest_sample_length\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  # Finally adapt the vectorize layer.\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  return vectorize_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8zDXrk9eEy5"
      },
      "source": [
        "## Define a dictionary with -> function_names:prepro_function_caller. And a dictionary to store model results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8owxNMIeFsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa577fa-2d32-4c6b-ff6b-a81b7a078bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STM_LOW\n",
            "STM_RSW\n",
            "LOW_STM_RSW\n",
            "LOW_RSW_STM\n",
            "STM_LOW_RSW\n",
            "STM_RSW_LOW\n",
            "RSW_LOW_STM\n",
            "RSW_STM_LOW\n"
          ]
        }
      ],
      "source": [
        "model_results = {}\n",
        "prepro_functions_dict_base = {\n",
        "    'DON':DON,\n",
        "    'LOW':LOW,\n",
        "    'RSW':RSW,\n",
        "    'STM':STM\n",
        "    }\n",
        "\n",
        "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
        "\n",
        "prepro_functions_dict_comb = {\n",
        "    # 1. Do nothing \n",
        "    #'DON': DON,\n",
        "    # 2. Lowercasing \n",
        "    #'LOW':LOW,\n",
        "    # 3. Removing Stopwords\n",
        "    #'RSW':RSW, \n",
        "    # 4. Porter Stemming\n",
        "    #'STM':STM,\n",
        "    # 5. LOW->RSW\n",
        "    #'LOW_RSW':LOW_RSW, \n",
        "    # 6. LOW->STM\n",
        "    #'LOW_STM':LOW_STM,\n",
        "    # 7. RSW->LOW\n",
        "    #'RSW_LOW':RSW_LOW,\n",
        "    # 8. RSW->STM\n",
        "    #'RSW_STM':RSW_STM,\n",
        "    # 9. STM->LOW\n",
        "    'STM_LOW':STM_LOW,\n",
        "    # 10. STM->RSW\n",
        "    'STM_RSW':STM_RSW,\n",
        "    # 11. LOW->STM->RSW\n",
        "    'LOW_STM_RSW':LOW_STM_RSW,  \n",
        "    # 12. LOW->RSW->STM\n",
        "    'LOW_RSW_STM':LOW_RSW_STM,\n",
        "    # 13. STM->LOW->RSW\n",
        "    'STM_LOW_RSW':STM_LOW_RSW,\n",
        "    # 14. STM->RSW->LOW\n",
        "    'STM_RSW_LOW':STM_RSW_LOW,\n",
        "    # 15. RSW->LOW->STM\n",
        "    'RSW_LOW_STM':RSW_LOW_STM,\n",
        "    # 16. RSW->STM->LOW\n",
        "    'RSW_STM_LOW':RSW_STM_LOW\n",
        "}\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  print(key)\n",
        "  model_results[key]=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some training hyperparameters...\n"
      ],
      "metadata": {
        "id": "TjxMtx0KaeqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 100\n",
        "\n",
        "num_runs = 5 \n",
        "# No need to go over the 20th epoch...Overfitting begins.\n",
        "num_epochs_per_run = 20\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()"
      ],
      "metadata": {
        "id": "la0vWhOAadHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDEi7MAo4qsQ"
      },
      "source": [
        "## Models definition and evaluation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm1fdCpETWL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a84b452-c8b3-4b3f-9cf1-27f95e8ee999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 2502\n",
            "Vocabulary size is: 94013\n",
            "5000/5000 [==============================] - 1339s 266ms/step - loss: 0.7008 - binary_accuracy: 0.4926 - val_loss: 0.6919 - val_binary_accuracy: 0.5192\n",
            "Run:  1 / Accuracy at epoch  0  is:  0.5192000269889832 \n",
            "\n",
            "5000/5000 [==============================] - 1292s 258ms/step - loss: 0.6908 - binary_accuracy: 0.5786 - val_loss: 0.6750 - val_binary_accuracy: 0.6710\n",
            "Run:  1 / Accuracy at epoch  1  is:  0.6710000038146973 \n",
            "\n",
            "5000/5000 [==============================] - 1302s 260ms/step - loss: 0.6500 - binary_accuracy: 0.6804 - val_loss: 0.5663 - val_binary_accuracy: 0.7572\n",
            "Run:  1 / Accuracy at epoch  2  is:  0.7572000026702881 \n",
            "\n",
            "5000/5000 [==============================] - 1295s 258ms/step - loss: 0.6112 - binary_accuracy: 0.7386 - val_loss: 0.5290 - val_binary_accuracy: 0.7778\n",
            "Run:  1 / Accuracy at epoch  3  is:  0.7778000235557556 \n",
            "\n",
            "5000/5000 [==============================] - 1314s 262ms/step - loss: 0.5666 - binary_accuracy: 0.7750 - val_loss: 0.5090 - val_binary_accuracy: 0.7880\n",
            "Run:  1 / Accuracy at epoch  4  is:  0.7879999876022339 \n",
            "\n",
            "5000/5000 [==============================] - 1323s 262ms/step - loss: 0.4807 - binary_accuracy: 0.8290 - val_loss: 0.7049 - val_binary_accuracy: 0.8236\n",
            "Run:  1 / Accuracy at epoch  5  is:  0.8235999941825867 \n",
            "\n",
            "5000/5000 [==============================] - 1312s 262ms/step - loss: 0.4230 - binary_accuracy: 0.8766 - val_loss: 0.6350 - val_binary_accuracy: 0.8228\n",
            "Run:  1 / Accuracy at epoch  6  is:  0.8227999806404114 \n",
            "\n",
            "5000/5000 [==============================] - 1296s 259ms/step - loss: 0.3692 - binary_accuracy: 0.9118 - val_loss: 1.0576 - val_binary_accuracy: 0.8094\n",
            "Run:  1 / Accuracy at epoch  7  is:  0.8094000220298767 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.3186 - binary_accuracy: 0.9274 - val_loss: 0.8979 - val_binary_accuracy: 0.8334\n",
            "Run:  1 / Accuracy at epoch  8  is:  0.8334000110626221 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.3121 - binary_accuracy: 0.9422 - val_loss: 1.3240 - val_binary_accuracy: 0.8126\n",
            "Run:  1 / Accuracy at epoch  9  is:  0.8126000165939331 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2921 - binary_accuracy: 0.9526 - val_loss: 1.3128 - val_binary_accuracy: 0.8348\n",
            "Run:  1 / Accuracy at epoch  10  is:  0.8348000049591064 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 256ms/step - loss: 0.2690 - binary_accuracy: 0.9592 - val_loss: 1.4863 - val_binary_accuracy: 0.8080\n",
            "Run:  1 / Accuracy at epoch  11  is:  0.8080000281333923 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.2457 - binary_accuracy: 0.9694 - val_loss: 1.5851 - val_binary_accuracy: 0.7874\n",
            "Run:  1 / Accuracy at epoch  12  is:  0.7874000072479248 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.2499 - binary_accuracy: 0.9736 - val_loss: 2.4898 - val_binary_accuracy: 0.8026\n",
            "Run:  1 / Accuracy at epoch  13  is:  0.8026000261306763 \n",
            "\n",
            "5000/5000 [==============================] - 1296s 259ms/step - loss: 0.2623 - binary_accuracy: 0.9776 - val_loss: 2.6847 - val_binary_accuracy: 0.8090\n",
            "Run:  1 / Accuracy at epoch  14  is:  0.8090000152587891 \n",
            "\n",
            "5000/5000 [==============================] - 1312s 262ms/step - loss: 0.1874 - binary_accuracy: 0.9828 - val_loss: 2.8204 - val_binary_accuracy: 0.8234\n",
            "Run:  1 / Accuracy at epoch  15  is:  0.8234000205993652 \n",
            "\n",
            "5000/5000 [==============================] - 1306s 261ms/step - loss: 0.2593 - binary_accuracy: 0.9854 - val_loss: 3.4079 - val_binary_accuracy: 0.7900\n",
            "Run:  1 / Accuracy at epoch  16  is:  0.7900000214576721 \n",
            "\n",
            "5000/5000 [==============================] - 1303s 260ms/step - loss: 0.2092 - binary_accuracy: 0.9872 - val_loss: 6.1510 - val_binary_accuracy: 0.7446\n",
            "Run:  1 / Accuracy at epoch  17  is:  0.7445999979972839 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.1534 - binary_accuracy: 0.9918 - val_loss: 3.7705 - val_binary_accuracy: 0.7824\n",
            "Run:  1 / Accuracy at epoch  18  is:  0.7824000120162964 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.1330 - binary_accuracy: 0.9900 - val_loss: 2.6996 - val_binary_accuracy: 0.8038\n",
            "Run:  1 / Accuracy at epoch  19  is:  0.8037999868392944 \n",
            "\n",
            "Accuracies over epochs: [0.5192000269889832, 0.6710000038146973, 0.7572000026702881, 0.7778000235557556, 0.7879999876022339, 0.8235999941825867, 0.8227999806404114, 0.8094000220298767, 0.8334000110626221, 0.8126000165939331, 0.8348000049591064, 0.8080000281333923, 0.7874000072479248, 0.8026000261306763, 0.8090000152587891, 0.8234000205993652, 0.7900000214576721, 0.7445999979972839, 0.7824000120162964, 0.8037999868392944] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 1271s 253ms/step - loss: 0.6944 - binary_accuracy: 0.5578 - val_loss: 0.6526 - val_binary_accuracy: 0.6016\n",
            "Run:  2 / Accuracy at epoch  0  is:  0.6015999913215637 \n",
            "\n",
            "5000/5000 [==============================] - 1272s 254ms/step - loss: 0.6268 - binary_accuracy: 0.6938 - val_loss: 0.5604 - val_binary_accuracy: 0.7194\n",
            "Run:  2 / Accuracy at epoch  1  is:  0.7193999886512756 \n",
            "\n",
            "5000/5000 [==============================] - 1279s 255ms/step - loss: 0.5708 - binary_accuracy: 0.7630 - val_loss: 0.6935 - val_binary_accuracy: 0.7706\n",
            "Run:  2 / Accuracy at epoch  2  is:  0.7706000208854675 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 256ms/step - loss: 0.5698 - binary_accuracy: 0.7872 - val_loss: 0.4949 - val_binary_accuracy: 0.7978\n",
            "Run:  2 / Accuracy at epoch  3  is:  0.7978000044822693 \n",
            "\n",
            "5000/5000 [==============================] - 1280s 255ms/step - loss: 0.4816 - binary_accuracy: 0.8328 - val_loss: 0.5775 - val_binary_accuracy: 0.8316\n",
            "Run:  2 / Accuracy at epoch  4  is:  0.83160001039505 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 255ms/step - loss: 0.3933 - binary_accuracy: 0.8924 - val_loss: 0.7635 - val_binary_accuracy: 0.8076\n",
            "Run:  2 / Accuracy at epoch  5  is:  0.8076000213623047 \n",
            "\n",
            "5000/5000 [==============================] - 1298s 259ms/step - loss: 0.3485 - binary_accuracy: 0.9142 - val_loss: 0.6239 - val_binary_accuracy: 0.8266\n",
            "Run:  2 / Accuracy at epoch  6  is:  0.8266000151634216 \n",
            "\n",
            "5000/5000 [==============================] - 1291s 255ms/step - loss: 0.3330 - binary_accuracy: 0.9254 - val_loss: 0.8505 - val_binary_accuracy: 0.8312\n",
            "Run:  2 / Accuracy at epoch  7  is:  0.8312000036239624 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.3394 - binary_accuracy: 0.9414 - val_loss: 1.2276 - val_binary_accuracy: 0.8326\n",
            "Run:  2 / Accuracy at epoch  8  is:  0.8325999975204468 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.3111 - binary_accuracy: 0.9522 - val_loss: 1.5623 - val_binary_accuracy: 0.7912\n",
            "Run:  2 / Accuracy at epoch  9  is:  0.7911999821662903 \n",
            "\n",
            "5000/5000 [==============================] - 1282s 256ms/step - loss: 0.2938 - binary_accuracy: 0.9570 - val_loss: 1.8857 - val_binary_accuracy: 0.7712\n",
            "Run:  2 / Accuracy at epoch  10  is:  0.7712000012397766 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2627 - binary_accuracy: 0.9676 - val_loss: 1.4340 - val_binary_accuracy: 0.8182\n",
            "Run:  2 / Accuracy at epoch  11  is:  0.8181999921798706 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.2474 - binary_accuracy: 0.9752 - val_loss: 2.0303 - val_binary_accuracy: 0.7316\n",
            "Run:  2 / Accuracy at epoch  12  is:  0.7315999865531921 \n",
            "\n",
            "5000/5000 [==============================] - 1295s 258ms/step - loss: 0.2088 - binary_accuracy: 0.9760 - val_loss: 4.3959 - val_binary_accuracy: 0.7370\n",
            "Run:  2 / Accuracy at epoch  13  is:  0.7369999885559082 \n",
            "\n",
            "5000/5000 [==============================] - 1282s 256ms/step - loss: 0.2122 - binary_accuracy: 0.9818 - val_loss: 3.2055 - val_binary_accuracy: 0.8120\n",
            "Run:  2 / Accuracy at epoch  14  is:  0.8119999766349792 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.2237 - binary_accuracy: 0.9840 - val_loss: 2.2709 - val_binary_accuracy: 0.8310\n",
            "Run:  2 / Accuracy at epoch  15  is:  0.8309999704360962 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2157 - binary_accuracy: 0.9880 - val_loss: 3.2460 - val_binary_accuracy: 0.8306\n",
            "Run:  2 / Accuracy at epoch  16  is:  0.8306000232696533 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.1846 - binary_accuracy: 0.9866 - val_loss: 3.1365 - val_binary_accuracy: 0.8238\n",
            "Run:  2 / Accuracy at epoch  17  is:  0.8238000273704529 \n",
            "\n",
            "5000/5000 [==============================] - 1287s 257ms/step - loss: 0.1682 - binary_accuracy: 0.9912 - val_loss: 4.1469 - val_binary_accuracy: 0.7504\n",
            "Run:  2 / Accuracy at epoch  18  is:  0.7504000067710876 \n",
            "\n",
            "5000/5000 [==============================] - 1286s 257ms/step - loss: 0.1941 - binary_accuracy: 0.9906 - val_loss: 5.0202 - val_binary_accuracy: 0.7650\n",
            "Run:  2 / Accuracy at epoch  19  is:  0.7649999856948853 \n",
            "\n",
            "Accuracies over epochs: [0.6015999913215637, 0.7193999886512756, 0.7706000208854675, 0.7978000044822693, 0.83160001039505, 0.8076000213623047, 0.8266000151634216, 0.8312000036239624, 0.8325999975204468, 0.7911999821662903, 0.7712000012397766, 0.8181999921798706, 0.7315999865531921, 0.7369999885559082, 0.8119999766349792, 0.8309999704360962, 0.8306000232696533, 0.8238000273704529, 0.7504000067710876, 0.7649999856948853] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 1264s 251ms/step - loss: 0.7020 - binary_accuracy: 0.4870 - val_loss: 0.6915 - val_binary_accuracy: 0.5358\n",
            "Run:  3 / Accuracy at epoch  0  is:  0.5357999801635742 \n",
            "\n",
            "5000/5000 [==============================] - 1266s 253ms/step - loss: 0.6948 - binary_accuracy: 0.5624 - val_loss: 0.6687 - val_binary_accuracy: 0.7016\n",
            "Run:  3 / Accuracy at epoch  1  is:  0.7016000151634216 \n",
            "\n",
            "5000/5000 [==============================] - 1279s 255ms/step - loss: 0.6468 - binary_accuracy: 0.6938 - val_loss: 0.5183 - val_binary_accuracy: 0.7758\n",
            "Run:  3 / Accuracy at epoch  2  is:  0.7757999897003174 \n",
            "\n",
            "5000/5000 [==============================] - 1288s 257ms/step - loss: 0.5948 - binary_accuracy: 0.7494 - val_loss: 0.5263 - val_binary_accuracy: 0.8012\n",
            "Run:  3 / Accuracy at epoch  3  is:  0.8011999726295471 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 256ms/step - loss: 0.5450 - binary_accuracy: 0.7914 - val_loss: 0.5494 - val_binary_accuracy: 0.8074\n",
            "Run:  3 / Accuracy at epoch  4  is:  0.8073999881744385 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 255ms/step - loss: 0.4628 - binary_accuracy: 0.8488 - val_loss: 0.7184 - val_binary_accuracy: 0.8290\n",
            "Run:  3 / Accuracy at epoch  5  is:  0.8289999961853027 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.3988 - binary_accuracy: 0.8900 - val_loss: 0.7742 - val_binary_accuracy: 0.8310\n",
            "Run:  3 / Accuracy at epoch  6  is:  0.8309999704360962 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.3663 - binary_accuracy: 0.9134 - val_loss: 0.7327 - val_binary_accuracy: 0.8404\n",
            "Run:  3 / Accuracy at epoch  7  is:  0.840399980545044 \n",
            "\n",
            "5000/5000 [==============================] - 1282s 256ms/step - loss: 0.3390 - binary_accuracy: 0.9310 - val_loss: 1.0336 - val_binary_accuracy: 0.8188\n",
            "Run:  3 / Accuracy at epoch  8  is:  0.8187999725341797 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.3303 - binary_accuracy: 0.9466 - val_loss: 1.3594 - val_binary_accuracy: 0.8340\n",
            "Run:  3 / Accuracy at epoch  9  is:  0.8339999914169312 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2910 - binary_accuracy: 0.9568 - val_loss: 1.7983 - val_binary_accuracy: 0.8138\n",
            "Run:  3 / Accuracy at epoch  10  is:  0.8137999773025513 \n",
            "\n",
            "5000/5000 [==============================] - 1295s 258ms/step - loss: 0.2600 - binary_accuracy: 0.9616 - val_loss: 3.1152 - val_binary_accuracy: 0.7544\n",
            "Run:  3 / Accuracy at epoch  11  is:  0.7544000148773193 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.2321 - binary_accuracy: 0.9728 - val_loss: 2.2185 - val_binary_accuracy: 0.7852\n",
            "Run:  3 / Accuracy at epoch  12  is:  0.7851999998092651 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.2188 - binary_accuracy: 0.9730 - val_loss: 1.7046 - val_binary_accuracy: 0.7922\n",
            "Run:  3 / Accuracy at epoch  13  is:  0.7922000288963318 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2167 - binary_accuracy: 0.9784 - val_loss: 2.5942 - val_binary_accuracy: 0.7774\n",
            "Run:  3 / Accuracy at epoch  14  is:  0.777400016784668 \n",
            "\n",
            "5000/5000 [==============================] - 1287s 257ms/step - loss: 0.1842 - binary_accuracy: 0.9826 - val_loss: 3.6007 - val_binary_accuracy: 0.6724\n",
            "Run:  3 / Accuracy at epoch  15  is:  0.6723999977111816 \n",
            "\n",
            "5000/5000 [==============================] - 1279s 255ms/step - loss: 0.1959 - binary_accuracy: 0.9780 - val_loss: 2.0817 - val_binary_accuracy: 0.7404\n",
            "Run:  3 / Accuracy at epoch  16  is:  0.7404000163078308 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.1930 - binary_accuracy: 0.9852 - val_loss: 2.1102 - val_binary_accuracy: 0.7668\n",
            "Run:  3 / Accuracy at epoch  17  is:  0.7667999863624573 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.1577 - binary_accuracy: 0.9856 - val_loss: 5.3072 - val_binary_accuracy: 0.7496\n",
            "Run:  3 / Accuracy at epoch  18  is:  0.7495999932289124 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.1405 - binary_accuracy: 0.9910 - val_loss: 3.3794 - val_binary_accuracy: 0.7976\n",
            "Run:  3 / Accuracy at epoch  19  is:  0.7975999712944031 \n",
            "\n",
            "Accuracies over epochs: [0.5357999801635742, 0.7016000151634216, 0.7757999897003174, 0.8011999726295471, 0.8073999881744385, 0.8289999961853027, 0.8309999704360962, 0.840399980545044, 0.8187999725341797, 0.8339999914169312, 0.8137999773025513, 0.7544000148773193, 0.7851999998092651, 0.7922000288963318, 0.777400016784668, 0.6723999977111816, 0.7404000163078308, 0.7667999863624573, 0.7495999932289124, 0.7975999712944031] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 1266s 252ms/step - loss: 0.7047 - binary_accuracy: 0.5012 - val_loss: 0.6887 - val_binary_accuracy: 0.5454\n",
            "Run:  4 / Accuracy at epoch  0  is:  0.5454000234603882 \n",
            "\n",
            "5000/5000 [==============================] - 1268s 253ms/step - loss: 0.6868 - binary_accuracy: 0.6024 - val_loss: 0.5702 - val_binary_accuracy: 0.7154\n",
            "Run:  4 / Accuracy at epoch  1  is:  0.715399980545044 \n",
            "\n",
            "5000/5000 [==============================] - 1282s 256ms/step - loss: 0.6259 - binary_accuracy: 0.7126 - val_loss: 0.6100 - val_binary_accuracy: 0.7122\n",
            "Run:  4 / Accuracy at epoch  2  is:  0.7121999859809875 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.5768 - binary_accuracy: 0.7608 - val_loss: 0.4874 - val_binary_accuracy: 0.7836\n",
            "Run:  4 / Accuracy at epoch  3  is:  0.7835999727249146 \n",
            "\n",
            "5000/5000 [==============================] - 1282s 256ms/step - loss: 0.5128 - binary_accuracy: 0.8138 - val_loss: 0.7322 - val_binary_accuracy: 0.8216\n",
            "Run:  4 / Accuracy at epoch  4  is:  0.8216000199317932 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.4481 - binary_accuracy: 0.8642 - val_loss: 0.6450 - val_binary_accuracy: 0.8394\n",
            "Run:  4 / Accuracy at epoch  5  is:  0.8393999934196472 \n",
            "\n",
            "5000/5000 [==============================] - 1287s 257ms/step - loss: 0.3877 - binary_accuracy: 0.8938 - val_loss: 0.7249 - val_binary_accuracy: 0.8420\n",
            "Run:  4 / Accuracy at epoch  6  is:  0.8420000076293945 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.3504 - binary_accuracy: 0.9168 - val_loss: 0.7381 - val_binary_accuracy: 0.8422\n",
            "Run:  4 / Accuracy at epoch  7  is:  0.842199981212616 \n",
            "\n",
            "5000/5000 [==============================] - 1301s 259ms/step - loss: 0.3489 - binary_accuracy: 0.9336 - val_loss: 1.2667 - val_binary_accuracy: 0.8012\n",
            "Run:  4 / Accuracy at epoch  8  is:  0.8011999726295471 \n",
            "\n",
            "5000/5000 [==============================] - 1297s 256ms/step - loss: 0.3274 - binary_accuracy: 0.9492 - val_loss: 1.6123 - val_binary_accuracy: 0.8250\n",
            "Run:  4 / Accuracy at epoch  9  is:  0.824999988079071 \n",
            "\n",
            "5000/5000 [==============================] - 1280s 255ms/step - loss: 0.3294 - binary_accuracy: 0.9558 - val_loss: 1.7929 - val_binary_accuracy: 0.7904\n",
            "Run:  4 / Accuracy at epoch  10  is:  0.7904000282287598 \n",
            "\n",
            "5000/5000 [==============================] - 1278s 255ms/step - loss: 0.3249 - binary_accuracy: 0.9634 - val_loss: 1.6605 - val_binary_accuracy: 0.8342\n",
            "Run:  4 / Accuracy at epoch  11  is:  0.8342000246047974 \n",
            "\n",
            "5000/5000 [==============================] - 1278s 255ms/step - loss: 0.3167 - binary_accuracy: 0.9636 - val_loss: 1.0924 - val_binary_accuracy: 0.8344\n",
            "Run:  4 / Accuracy at epoch  12  is:  0.8343999981880188 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.2757 - binary_accuracy: 0.9698 - val_loss: 1.5486 - val_binary_accuracy: 0.8048\n",
            "Run:  4 / Accuracy at epoch  13  is:  0.8047999739646912 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.2263 - binary_accuracy: 0.9788 - val_loss: 2.4148 - val_binary_accuracy: 0.8318\n",
            "Run:  4 / Accuracy at epoch  14  is:  0.8317999839782715 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.2303 - binary_accuracy: 0.9796 - val_loss: 2.2136 - val_binary_accuracy: 0.8256\n",
            "Run:  4 / Accuracy at epoch  15  is:  0.8256000280380249 \n",
            "\n",
            "5000/5000 [==============================] - 1279s 255ms/step - loss: 0.1757 - binary_accuracy: 0.9826 - val_loss: 3.5131 - val_binary_accuracy: 0.7856\n",
            "Run:  4 / Accuracy at epoch  16  is:  0.7856000065803528 \n",
            "\n",
            "5000/5000 [==============================] - 1298s 259ms/step - loss: 0.1663 - binary_accuracy: 0.9862 - val_loss: 3.6016 - val_binary_accuracy: 0.7892\n",
            "Run:  4 / Accuracy at epoch  17  is:  0.7892000079154968 \n",
            "\n",
            "5000/5000 [==============================] - 1315s 262ms/step - loss: 0.1649 - binary_accuracy: 0.9904 - val_loss: 3.1340 - val_binary_accuracy: 0.8172\n",
            "Run:  4 / Accuracy at epoch  18  is:  0.8172000050544739 \n",
            "\n",
            "5000/5000 [==============================] - 1307s 261ms/step - loss: 0.1312 - binary_accuracy: 0.9912 - val_loss: 3.8753 - val_binary_accuracy: 0.8340\n",
            "Run:  4 / Accuracy at epoch  19  is:  0.8339999914169312 \n",
            "\n",
            "Accuracies over epochs: [0.5454000234603882, 0.715399980545044, 0.7121999859809875, 0.7835999727249146, 0.8216000199317932, 0.8393999934196472, 0.8420000076293945, 0.842199981212616, 0.8011999726295471, 0.824999988079071, 0.7904000282287598, 0.8342000246047974, 0.8343999981880188, 0.8047999739646912, 0.8317999839782715, 0.8256000280380249, 0.7856000065803528, 0.7892000079154968, 0.8172000050544739, 0.8339999914169312] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 1279s 254ms/step - loss: 0.7030 - binary_accuracy: 0.4998 - val_loss: 0.6804 - val_binary_accuracy: 0.5648\n",
            "Run:  5 / Accuracy at epoch  0  is:  0.5648000240325928 \n",
            "\n",
            "5000/5000 [==============================] - 1284s 256ms/step - loss: 0.6834 - binary_accuracy: 0.6068 - val_loss: 0.6067 - val_binary_accuracy: 0.6654\n",
            "Run:  5 / Accuracy at epoch  1  is:  0.6654000282287598 \n",
            "\n",
            "5000/5000 [==============================] - 1289s 257ms/step - loss: 0.6125 - binary_accuracy: 0.7308 - val_loss: 0.5558 - val_binary_accuracy: 0.7592\n",
            "Run:  5 / Accuracy at epoch  2  is:  0.7591999769210815 \n",
            "\n",
            "5000/5000 [==============================] - 1285s 256ms/step - loss: 0.5590 - binary_accuracy: 0.7778 - val_loss: 0.6197 - val_binary_accuracy: 0.7846\n",
            "Run:  5 / Accuracy at epoch  3  is:  0.784600019454956 \n",
            "\n",
            "5000/5000 [==============================] - 1280s 255ms/step - loss: 0.5297 - binary_accuracy: 0.8162 - val_loss: 0.4930 - val_binary_accuracy: 0.7896\n",
            "Run:  5 / Accuracy at epoch  4  is:  0.7896000146865845 \n",
            "\n",
            "5000/5000 [==============================] - 1276s 255ms/step - loss: 0.4516 - binary_accuracy: 0.8518 - val_loss: 0.6235 - val_binary_accuracy: 0.8274\n",
            "Run:  5 / Accuracy at epoch  5  is:  0.8274000287055969 \n",
            "\n",
            "5000/5000 [==============================] - 1274s 254ms/step - loss: 0.4114 - binary_accuracy: 0.8786 - val_loss: 0.7699 - val_binary_accuracy: 0.8362\n",
            "Run:  5 / Accuracy at epoch  6  is:  0.8361999988555908 \n",
            "\n",
            "5000/5000 [==============================] - 1275s 254ms/step - loss: 0.3692 - binary_accuracy: 0.9078 - val_loss: 0.7439 - val_binary_accuracy: 0.8398\n",
            "Run:  5 / Accuracy at epoch  7  is:  0.8398000001907349 \n",
            "\n",
            "5000/5000 [==============================] - 1279s 255ms/step - loss: 0.3345 - binary_accuracy: 0.9246 - val_loss: 1.0451 - val_binary_accuracy: 0.8426\n",
            "Run:  5 / Accuracy at epoch  8  is:  0.8425999879837036 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 256ms/step - loss: 0.3118 - binary_accuracy: 0.9418 - val_loss: 1.2488 - val_binary_accuracy: 0.8212\n",
            "Run:  5 / Accuracy at epoch  9  is:  0.8212000131607056 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2738 - binary_accuracy: 0.9536 - val_loss: 1.0348 - val_binary_accuracy: 0.8348\n",
            "Run:  5 / Accuracy at epoch  10  is:  0.8348000049591064 \n",
            "\n",
            "5000/5000 [==============================] - 1283s 256ms/step - loss: 0.2502 - binary_accuracy: 0.9642 - val_loss: 1.1400 - val_binary_accuracy: 0.8144\n",
            "Run:  5 / Accuracy at epoch  11  is:  0.8144000172615051 \n",
            "\n",
            "5000/5000 [==============================] - 1277s 255ms/step - loss: 0.2530 - binary_accuracy: 0.9742 - val_loss: 2.0264 - val_binary_accuracy: 0.8160\n",
            "Run:  5 / Accuracy at epoch  12  is:  0.8159999847412109 \n",
            "\n",
            "5000/5000 [==============================] - 1281s 255ms/step - loss: 0.2347 - binary_accuracy: 0.9772 - val_loss: 1.6039 - val_binary_accuracy: 0.8180\n",
            "Run:  5 / Accuracy at epoch  13  is:  0.8180000185966492 \n",
            "\n",
            "5000/5000 [==============================] - 1303s 258ms/step - loss: 0.1635 - binary_accuracy: 0.9832 - val_loss: 3.8003 - val_binary_accuracy: 0.7976\n",
            "Run:  5 / Accuracy at epoch  14  is:  0.7975999712944031 \n",
            "\n",
            "5000/5000 [==============================] - 1277s 255ms/step - loss: 0.1573 - binary_accuracy: 0.9880 - val_loss: 3.6570 - val_binary_accuracy: 0.8056\n",
            "Run:  5 / Accuracy at epoch  15  is:  0.8055999875068665 \n",
            "\n",
            "5000/5000 [==============================] - 1276s 255ms/step - loss: 0.1911 - binary_accuracy: 0.9898 - val_loss: 3.6477 - val_binary_accuracy: 0.8248\n",
            "Run:  5 / Accuracy at epoch  16  is:  0.8248000144958496 \n",
            "\n",
            "5000/5000 [==============================] - 1277s 255ms/step - loss: 0.1440 - binary_accuracy: 0.9906 - val_loss: 6.7940 - val_binary_accuracy: 0.7546\n",
            "Run:  5 / Accuracy at epoch  17  is:  0.7545999884605408 \n",
            "\n",
            "5000/5000 [==============================] - 1274s 254ms/step - loss: 0.1418 - binary_accuracy: 0.9900 - val_loss: 3.5573 - val_binary_accuracy: 0.8300\n",
            "Run:  5 / Accuracy at epoch  18  is:  0.8299999833106995 \n",
            "\n",
            "5000/5000 [==============================] - 1273s 254ms/step - loss: 0.1075 - binary_accuracy: 0.9922 - val_loss: 5.8689 - val_binary_accuracy: 0.8014\n",
            "Run:  5 / Accuracy at epoch  19  is:  0.8014000058174133 \n",
            "\n",
            "Accuracies over epochs: [0.5648000240325928, 0.6654000282287598, 0.7591999769210815, 0.784600019454956, 0.7896000146865845, 0.8274000287055969, 0.8361999988555908, 0.8398000001907349, 0.8425999879837036, 0.8212000131607056, 0.8348000049591064, 0.8144000172615051, 0.8159999847412109, 0.8180000185966492, 0.7975999712944031, 0.8055999875068665, 0.8248000144958496, 0.7545999884605408, 0.8299999833106995, 0.8014000058174133] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Over all runs maximum accuracies are: [0.8325999975204468, 0.8348000049591064, 0.840399980545044, 0.842199981212616, 0.8425999879837036]\n",
            "The median is: 0.840399980545044 \n",
            "\n",
            "\n",
            "\n",
            "BiLSTM Accuracy Score on Test set ->  ['0.840399980545044 +/- 0.007799983024597168']\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 1533\n",
            "Vocabulary size is: 103029\n",
            "5000/5000 [==============================] - 798s 158ms/step - loss: 0.7022 - binary_accuracy: 0.5058 - val_loss: 0.6842 - val_binary_accuracy: 0.5598\n",
            "Run:  1 / Accuracy at epoch  0  is:  0.5598000288009644 \n",
            "\n",
            "5000/5000 [==============================] - 804s 160ms/step - loss: 0.6897 - binary_accuracy: 0.5926 - val_loss: 0.6027 - val_binary_accuracy: 0.6904\n",
            "Run:  1 / Accuracy at epoch  1  is:  0.6904000043869019 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.6067 - binary_accuracy: 0.7348 - val_loss: 0.5811 - val_binary_accuracy: 0.7534\n",
            "Run:  1 / Accuracy at epoch  2  is:  0.7534000277519226 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.5602 - binary_accuracy: 0.7780 - val_loss: 0.5656 - val_binary_accuracy: 0.8062\n",
            "Run:  1 / Accuracy at epoch  3  is:  0.8062000274658203 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.5301 - binary_accuracy: 0.8032 - val_loss: 0.6231 - val_binary_accuracy: 0.8104\n",
            "Run:  1 / Accuracy at epoch  4  is:  0.8104000091552734 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.4442 - binary_accuracy: 0.8568 - val_loss: 0.5799 - val_binary_accuracy: 0.8440\n",
            "Run:  1 / Accuracy at epoch  5  is:  0.843999981880188 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.3868 - binary_accuracy: 0.8966 - val_loss: 0.5984 - val_binary_accuracy: 0.8320\n",
            "Run:  1 / Accuracy at epoch  6  is:  0.8320000171661377 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.3335 - binary_accuracy: 0.9250 - val_loss: 0.8167 - val_binary_accuracy: 0.8408\n",
            "Run:  1 / Accuracy at epoch  7  is:  0.8407999873161316 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.3069 - binary_accuracy: 0.9404 - val_loss: 0.9423 - val_binary_accuracy: 0.8298\n",
            "Run:  1 / Accuracy at epoch  8  is:  0.829800009727478 \n",
            "\n",
            "5000/5000 [==============================] - 812s 162ms/step - loss: 0.2863 - binary_accuracy: 0.9544 - val_loss: 1.3002 - val_binary_accuracy: 0.8148\n",
            "Run:  1 / Accuracy at epoch  9  is:  0.8148000240325928 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.2697 - binary_accuracy: 0.9596 - val_loss: 1.0276 - val_binary_accuracy: 0.7950\n",
            "Run:  1 / Accuracy at epoch  10  is:  0.7950000166893005 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.2273 - binary_accuracy: 0.9710 - val_loss: 2.1485 - val_binary_accuracy: 0.7708\n",
            "Run:  1 / Accuracy at epoch  11  is:  0.770799994468689 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.2290 - binary_accuracy: 0.9750 - val_loss: 1.6520 - val_binary_accuracy: 0.7706\n",
            "Run:  1 / Accuracy at epoch  12  is:  0.7706000208854675 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1954 - binary_accuracy: 0.9796 - val_loss: 2.6356 - val_binary_accuracy: 0.7802\n",
            "Run:  1 / Accuracy at epoch  13  is:  0.7802000045776367 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.2129 - binary_accuracy: 0.9826 - val_loss: 2.4593 - val_binary_accuracy: 0.7538\n",
            "Run:  1 / Accuracy at epoch  14  is:  0.7537999749183655 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.1699 - binary_accuracy: 0.9848 - val_loss: 2.6252 - val_binary_accuracy: 0.7874\n",
            "Run:  1 / Accuracy at epoch  15  is:  0.7874000072479248 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.1462 - binary_accuracy: 0.9832 - val_loss: 2.5177 - val_binary_accuracy: 0.7928\n",
            "Run:  1 / Accuracy at epoch  16  is:  0.7928000092506409 \n",
            "\n",
            "5000/5000 [==============================] - 812s 162ms/step - loss: 0.1423 - binary_accuracy: 0.9884 - val_loss: 3.6911 - val_binary_accuracy: 0.7756\n",
            "Run:  1 / Accuracy at epoch  17  is:  0.775600016117096 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1396 - binary_accuracy: 0.9896 - val_loss: 4.4748 - val_binary_accuracy: 0.7902\n",
            "Run:  1 / Accuracy at epoch  18  is:  0.7901999950408936 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1526 - binary_accuracy: 0.9902 - val_loss: 4.2118 - val_binary_accuracy: 0.8086\n",
            "Run:  1 / Accuracy at epoch  19  is:  0.8086000084877014 \n",
            "\n",
            "Accuracies over epochs: [0.5598000288009644, 0.6904000043869019, 0.7534000277519226, 0.8062000274658203, 0.8104000091552734, 0.843999981880188, 0.8320000171661377, 0.8407999873161316, 0.829800009727478, 0.8148000240325928, 0.7950000166893005, 0.770799994468689, 0.7706000208854675, 0.7802000045776367, 0.7537999749183655, 0.7874000072479248, 0.7928000092506409, 0.775600016117096, 0.7901999950408936, 0.8086000084877014] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 802s 159ms/step - loss: 0.6968 - binary_accuracy: 0.5484 - val_loss: 0.7292 - val_binary_accuracy: 0.7100\n",
            "Run:  2 / Accuracy at epoch  0  is:  0.7099999785423279 \n",
            "\n",
            "5000/5000 [==============================] - 799s 159ms/step - loss: 0.6259 - binary_accuracy: 0.7054 - val_loss: 0.5909 - val_binary_accuracy: 0.7802\n",
            "Run:  2 / Accuracy at epoch  1  is:  0.7802000045776367 \n",
            "\n",
            "5000/5000 [==============================] - 801s 160ms/step - loss: 0.5627 - binary_accuracy: 0.7750 - val_loss: 0.5678 - val_binary_accuracy: 0.7254\n",
            "Run:  2 / Accuracy at epoch  2  is:  0.7253999710083008 \n",
            "\n",
            "5000/5000 [==============================] - 807s 161ms/step - loss: 0.5287 - binary_accuracy: 0.8024 - val_loss: 0.7928 - val_binary_accuracy: 0.7930\n",
            "Run:  2 / Accuracy at epoch  3  is:  0.7929999828338623 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.4797 - binary_accuracy: 0.8458 - val_loss: 0.6241 - val_binary_accuracy: 0.8238\n",
            "Run:  2 / Accuracy at epoch  4  is:  0.8238000273704529 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.4111 - binary_accuracy: 0.8812 - val_loss: 0.8222 - val_binary_accuracy: 0.8424\n",
            "Run:  2 / Accuracy at epoch  5  is:  0.8424000144004822 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.3285 - binary_accuracy: 0.9166 - val_loss: 0.8401 - val_binary_accuracy: 0.8326\n",
            "Run:  2 / Accuracy at epoch  6  is:  0.8325999975204468 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.3131 - binary_accuracy: 0.9350 - val_loss: 0.7717 - val_binary_accuracy: 0.8436\n",
            "Run:  2 / Accuracy at epoch  7  is:  0.8435999751091003 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.2974 - binary_accuracy: 0.9486 - val_loss: 1.0918 - val_binary_accuracy: 0.8100\n",
            "Run:  2 / Accuracy at epoch  8  is:  0.8100000023841858 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2914 - binary_accuracy: 0.9556 - val_loss: 1.2146 - val_binary_accuracy: 0.8372\n",
            "Run:  2 / Accuracy at epoch  9  is:  0.8371999859809875 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.3160 - binary_accuracy: 0.9632 - val_loss: 1.1405 - val_binary_accuracy: 0.8378\n",
            "Run:  2 / Accuracy at epoch  10  is:  0.8378000259399414 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2738 - binary_accuracy: 0.9716 - val_loss: 2.4051 - val_binary_accuracy: 0.7950\n",
            "Run:  2 / Accuracy at epoch  11  is:  0.7950000166893005 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.2381 - binary_accuracy: 0.9724 - val_loss: 1.9213 - val_binary_accuracy: 0.8108\n",
            "Run:  2 / Accuracy at epoch  12  is:  0.8108000159263611 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.2143 - binary_accuracy: 0.9810 - val_loss: 4.6811 - val_binary_accuracy: 0.7238\n",
            "Run:  2 / Accuracy at epoch  13  is:  0.723800003528595 \n",
            "\n",
            "5000/5000 [==============================] - 817s 163ms/step - loss: 0.2264 - binary_accuracy: 0.9828 - val_loss: 2.4839 - val_binary_accuracy: 0.7870\n",
            "Run:  2 / Accuracy at epoch  14  is:  0.7870000004768372 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.2058 - binary_accuracy: 0.9850 - val_loss: 2.9017 - val_binary_accuracy: 0.7630\n",
            "Run:  2 / Accuracy at epoch  15  is:  0.7630000114440918 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.1634 - binary_accuracy: 0.9898 - val_loss: 3.1414 - val_binary_accuracy: 0.8116\n",
            "Run:  2 / Accuracy at epoch  16  is:  0.8116000294685364 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1844 - binary_accuracy: 0.9894 - val_loss: 4.0812 - val_binary_accuracy: 0.7980\n",
            "Run:  2 / Accuracy at epoch  17  is:  0.7979999780654907 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.1242 - binary_accuracy: 0.9932 - val_loss: 5.3124 - val_binary_accuracy: 0.7854\n",
            "Run:  2 / Accuracy at epoch  18  is:  0.7853999733924866 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.1398 - binary_accuracy: 0.9910 - val_loss: 2.6696 - val_binary_accuracy: 0.8038\n",
            "Run:  2 / Accuracy at epoch  19  is:  0.8037999868392944 \n",
            "\n",
            "Accuracies over epochs: [0.7099999785423279, 0.7802000045776367, 0.7253999710083008, 0.7929999828338623, 0.8238000273704529, 0.8424000144004822, 0.8325999975204468, 0.8435999751091003, 0.8100000023841858, 0.8371999859809875, 0.8378000259399414, 0.7950000166893005, 0.8108000159263611, 0.723800003528595, 0.7870000004768372, 0.7630000114440918, 0.8116000294685364, 0.7979999780654907, 0.7853999733924866, 0.8037999868392944] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 805s 160ms/step - loss: 0.7013 - binary_accuracy: 0.5320 - val_loss: 0.6551 - val_binary_accuracy: 0.6692\n",
            "Run:  3 / Accuracy at epoch  0  is:  0.6692000031471252 \n",
            "\n",
            "5000/5000 [==============================] - 804s 160ms/step - loss: 0.6457 - binary_accuracy: 0.6874 - val_loss: 0.5690 - val_binary_accuracy: 0.7450\n",
            "Run:  3 / Accuracy at epoch  1  is:  0.7450000047683716 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.5919 - binary_accuracy: 0.7440 - val_loss: 0.6662 - val_binary_accuracy: 0.6876\n",
            "Run:  3 / Accuracy at epoch  2  is:  0.6876000165939331 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.6036 - binary_accuracy: 0.7496 - val_loss: 0.5007 - val_binary_accuracy: 0.7836\n",
            "Run:  3 / Accuracy at epoch  3  is:  0.7835999727249146 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.4999 - binary_accuracy: 0.8268 - val_loss: 0.5083 - val_binary_accuracy: 0.8142\n",
            "Run:  3 / Accuracy at epoch  4  is:  0.8141999840736389 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.3926 - binary_accuracy: 0.8818 - val_loss: 0.6960 - val_binary_accuracy: 0.8288\n",
            "Run:  3 / Accuracy at epoch  5  is:  0.8288000226020813 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.3347 - binary_accuracy: 0.9118 - val_loss: 0.7115 - val_binary_accuracy: 0.8336\n",
            "Run:  3 / Accuracy at epoch  6  is:  0.8335999846458435 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.2885 - binary_accuracy: 0.9328 - val_loss: 0.7890 - val_binary_accuracy: 0.8270\n",
            "Run:  3 / Accuracy at epoch  7  is:  0.8270000219345093 \n",
            "\n",
            "5000/5000 [==============================] - 812s 162ms/step - loss: 0.2669 - binary_accuracy: 0.9448 - val_loss: 0.7519 - val_binary_accuracy: 0.8036\n",
            "Run:  3 / Accuracy at epoch  8  is:  0.803600013256073 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.2394 - binary_accuracy: 0.9542 - val_loss: 1.6204 - val_binary_accuracy: 0.8046\n",
            "Run:  3 / Accuracy at epoch  9  is:  0.8046000003814697 \n",
            "\n",
            "5000/5000 [==============================] - 827s 163ms/step - loss: 0.2307 - binary_accuracy: 0.9650 - val_loss: 2.4597 - val_binary_accuracy: 0.7808\n",
            "Run:  3 / Accuracy at epoch  10  is:  0.7807999849319458 \n",
            "\n",
            "5000/5000 [==============================] - 817s 161ms/step - loss: 0.2266 - binary_accuracy: 0.9768 - val_loss: 2.5605 - val_binary_accuracy: 0.7746\n",
            "Run:  3 / Accuracy at epoch  11  is:  0.7746000289916992 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1982 - binary_accuracy: 0.9780 - val_loss: 3.6216 - val_binary_accuracy: 0.7576\n",
            "Run:  3 / Accuracy at epoch  12  is:  0.7576000094413757 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.1690 - binary_accuracy: 0.9808 - val_loss: 3.1014 - val_binary_accuracy: 0.7676\n",
            "Run:  3 / Accuracy at epoch  13  is:  0.7675999999046326 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1477 - binary_accuracy: 0.9866 - val_loss: 3.3563 - val_binary_accuracy: 0.7704\n",
            "Run:  3 / Accuracy at epoch  14  is:  0.7703999876976013 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1429 - binary_accuracy: 0.9910 - val_loss: 2.9156 - val_binary_accuracy: 0.7934\n",
            "Run:  3 / Accuracy at epoch  15  is:  0.79339998960495 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.1252 - binary_accuracy: 0.9892 - val_loss: 4.7753 - val_binary_accuracy: 0.7732\n",
            "Run:  3 / Accuracy at epoch  16  is:  0.7731999754905701 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.1147 - binary_accuracy: 0.9928 - val_loss: 5.8301 - val_binary_accuracy: 0.7266\n",
            "Run:  3 / Accuracy at epoch  17  is:  0.7265999913215637 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.0884 - binary_accuracy: 0.9948 - val_loss: 6.7316 - val_binary_accuracy: 0.7550\n",
            "Run:  3 / Accuracy at epoch  18  is:  0.7549999952316284 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.1055 - binary_accuracy: 0.9920 - val_loss: 4.2161 - val_binary_accuracy: 0.7914\n",
            "Run:  3 / Accuracy at epoch  19  is:  0.7914000153541565 \n",
            "\n",
            "Accuracies over epochs: [0.6692000031471252, 0.7450000047683716, 0.6876000165939331, 0.7835999727249146, 0.8141999840736389, 0.8288000226020813, 0.8335999846458435, 0.8270000219345093, 0.803600013256073, 0.8046000003814697, 0.7807999849319458, 0.7746000289916992, 0.7576000094413757, 0.7675999999046326, 0.7703999876976013, 0.79339998960495, 0.7731999754905701, 0.7265999913215637, 0.7549999952316284, 0.7914000153541565] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 801s 159ms/step - loss: 0.7025 - binary_accuracy: 0.4900 - val_loss: 0.6936 - val_binary_accuracy: 0.5016\n",
            "Run:  4 / Accuracy at epoch  0  is:  0.5016000270843506 \n",
            "\n",
            "5000/5000 [==============================] - 801s 160ms/step - loss: 0.6993 - binary_accuracy: 0.5292 - val_loss: 0.6979 - val_binary_accuracy: 0.5542\n",
            "Run:  4 / Accuracy at epoch  1  is:  0.5541999936103821 \n",
            "\n",
            "5000/5000 [==============================] - 820s 163ms/step - loss: 0.6870 - binary_accuracy: 0.6202 - val_loss: 0.6045 - val_binary_accuracy: 0.7018\n",
            "Run:  4 / Accuracy at epoch  2  is:  0.7017999887466431 \n",
            "\n",
            "5000/5000 [==============================] - 821s 163ms/step - loss: 0.6216 - binary_accuracy: 0.7194 - val_loss: 0.7804 - val_binary_accuracy: 0.5516\n",
            "Run:  4 / Accuracy at epoch  3  is:  0.5515999794006348 \n",
            "\n",
            "5000/5000 [==============================] - 826s 164ms/step - loss: 0.5804 - binary_accuracy: 0.7622 - val_loss: 0.5745 - val_binary_accuracy: 0.7926\n",
            "Run:  4 / Accuracy at epoch  4  is:  0.7925999760627747 \n",
            "\n",
            "5000/5000 [==============================] - 826s 165ms/step - loss: 0.5272 - binary_accuracy: 0.8054 - val_loss: 0.6745 - val_binary_accuracy: 0.8168\n",
            "Run:  4 / Accuracy at epoch  5  is:  0.8167999982833862 \n",
            "\n",
            "5000/5000 [==============================] - 825s 164ms/step - loss: 0.4584 - binary_accuracy: 0.8520 - val_loss: 0.6690 - val_binary_accuracy: 0.8312\n",
            "Run:  4 / Accuracy at epoch  6  is:  0.8312000036239624 \n",
            "\n",
            "5000/5000 [==============================] - 824s 164ms/step - loss: 0.3955 - binary_accuracy: 0.8882 - val_loss: 1.0476 - val_binary_accuracy: 0.8302\n",
            "Run:  4 / Accuracy at epoch  7  is:  0.8302000164985657 \n",
            "\n",
            "5000/5000 [==============================] - 826s 165ms/step - loss: 0.3658 - binary_accuracy: 0.9200 - val_loss: 0.8897 - val_binary_accuracy: 0.8404\n",
            "Run:  4 / Accuracy at epoch  8  is:  0.840399980545044 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.3159 - binary_accuracy: 0.9358 - val_loss: 1.6079 - val_binary_accuracy: 0.8102\n",
            "Run:  4 / Accuracy at epoch  9  is:  0.8101999759674072 \n",
            "\n",
            "5000/5000 [==============================] - 811s 162ms/step - loss: 0.2557 - binary_accuracy: 0.9484 - val_loss: 1.5845 - val_binary_accuracy: 0.7658\n",
            "Run:  4 / Accuracy at epoch  10  is:  0.7657999992370605 \n",
            "\n",
            "5000/5000 [==============================] - 812s 162ms/step - loss: 0.2357 - binary_accuracy: 0.9602 - val_loss: 1.5106 - val_binary_accuracy: 0.8320\n",
            "Run:  4 / Accuracy at epoch  11  is:  0.8320000171661377 \n",
            "\n",
            "5000/5000 [==============================] - 818s 163ms/step - loss: 0.2191 - binary_accuracy: 0.9678 - val_loss: 1.9292 - val_binary_accuracy: 0.7960\n",
            "Run:  4 / Accuracy at epoch  12  is:  0.7960000038146973 \n",
            "\n",
            "5000/5000 [==============================] - 816s 163ms/step - loss: 0.1958 - binary_accuracy: 0.9730 - val_loss: 2.0442 - val_binary_accuracy: 0.7752\n",
            "Run:  4 / Accuracy at epoch  13  is:  0.7752000093460083 \n",
            "\n",
            "5000/5000 [==============================] - 822s 164ms/step - loss: 0.1690 - binary_accuracy: 0.9798 - val_loss: 2.9450 - val_binary_accuracy: 0.7870\n",
            "Run:  4 / Accuracy at epoch  14  is:  0.7870000004768372 \n",
            "\n",
            "5000/5000 [==============================] - 820s 163ms/step - loss: 0.1853 - binary_accuracy: 0.9804 - val_loss: 6.2129 - val_binary_accuracy: 0.6780\n",
            "Run:  4 / Accuracy at epoch  15  is:  0.6779999732971191 \n",
            "\n",
            "5000/5000 [==============================] - 820s 163ms/step - loss: 0.1820 - binary_accuracy: 0.9866 - val_loss: 5.1329 - val_binary_accuracy: 0.7050\n",
            "Run:  4 / Accuracy at epoch  16  is:  0.7049999833106995 \n",
            "\n",
            "5000/5000 [==============================] - 823s 164ms/step - loss: 0.1721 - binary_accuracy: 0.9838 - val_loss: 5.4398 - val_binary_accuracy: 0.7436\n",
            "Run:  4 / Accuracy at epoch  17  is:  0.7436000108718872 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.1320 - binary_accuracy: 0.9886 - val_loss: 5.2113 - val_binary_accuracy: 0.7552\n",
            "Run:  4 / Accuracy at epoch  18  is:  0.7552000284194946 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.1387 - binary_accuracy: 0.9914 - val_loss: 5.1434 - val_binary_accuracy: 0.6956\n",
            "Run:  4 / Accuracy at epoch  19  is:  0.6955999732017517 \n",
            "\n",
            "Accuracies over epochs: [0.5016000270843506, 0.5541999936103821, 0.7017999887466431, 0.5515999794006348, 0.7925999760627747, 0.8167999982833862, 0.8312000036239624, 0.8302000164985657, 0.840399980545044, 0.8101999759674072, 0.7657999992370605, 0.8320000171661377, 0.7960000038146973, 0.7752000093460083, 0.7870000004768372, 0.6779999732971191, 0.7049999833106995, 0.7436000108718872, 0.7552000284194946, 0.6955999732017517] \n",
            "\n",
            "\n",
            "5000/5000 [==============================] - 797s 158ms/step - loss: 0.7000 - binary_accuracy: 0.5006 - val_loss: 0.6807 - val_binary_accuracy: 0.5590\n",
            "Run:  5 / Accuracy at epoch  0  is:  0.5590000152587891 \n",
            "\n",
            "5000/5000 [==============================] - 802s 160ms/step - loss: 0.6938 - binary_accuracy: 0.5726 - val_loss: 0.6194 - val_binary_accuracy: 0.6768\n",
            "Run:  5 / Accuracy at epoch  1  is:  0.676800012588501 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.6215 - binary_accuracy: 0.6984 - val_loss: 0.5673 - val_binary_accuracy: 0.7224\n",
            "Run:  5 / Accuracy at epoch  2  is:  0.7224000096321106 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.5799 - binary_accuracy: 0.7502 - val_loss: 0.5154 - val_binary_accuracy: 0.7882\n",
            "Run:  5 / Accuracy at epoch  3  is:  0.7882000207901001 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.4904 - binary_accuracy: 0.8208 - val_loss: 0.5574 - val_binary_accuracy: 0.8196\n",
            "Run:  5 / Accuracy at epoch  4  is:  0.819599986076355 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.4225 - binary_accuracy: 0.8686 - val_loss: 0.5233 - val_binary_accuracy: 0.8336\n",
            "Run:  5 / Accuracy at epoch  5  is:  0.8335999846458435 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.3609 - binary_accuracy: 0.9028 - val_loss: 0.5676 - val_binary_accuracy: 0.8466\n",
            "Run:  5 / Accuracy at epoch  6  is:  0.8465999960899353 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.3456 - binary_accuracy: 0.9244 - val_loss: 0.9459 - val_binary_accuracy: 0.8404\n",
            "Run:  5 / Accuracy at epoch  7  is:  0.840399980545044 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.2998 - binary_accuracy: 0.9426 - val_loss: 0.9314 - val_binary_accuracy: 0.8428\n",
            "Run:  5 / Accuracy at epoch  8  is:  0.8428000211715698 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2734 - binary_accuracy: 0.9512 - val_loss: 1.8046 - val_binary_accuracy: 0.7492\n",
            "Run:  5 / Accuracy at epoch  9  is:  0.7491999864578247 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2942 - binary_accuracy: 0.9582 - val_loss: 1.2798 - val_binary_accuracy: 0.7948\n",
            "Run:  5 / Accuracy at epoch  10  is:  0.7947999835014343 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2426 - binary_accuracy: 0.9688 - val_loss: 1.8857 - val_binary_accuracy: 0.7936\n",
            "Run:  5 / Accuracy at epoch  11  is:  0.7936000227928162 \n",
            "\n",
            "5000/5000 [==============================] - 810s 162ms/step - loss: 0.2462 - binary_accuracy: 0.9734 - val_loss: 1.4415 - val_binary_accuracy: 0.8206\n",
            "Run:  5 / Accuracy at epoch  12  is:  0.8205999732017517 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2334 - binary_accuracy: 0.9770 - val_loss: 2.0297 - val_binary_accuracy: 0.8254\n",
            "Run:  5 / Accuracy at epoch  13  is:  0.8253999948501587 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2236 - binary_accuracy: 0.9796 - val_loss: 1.5981 - val_binary_accuracy: 0.8176\n",
            "Run:  5 / Accuracy at epoch  14  is:  0.8176000118255615 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.2057 - binary_accuracy: 0.9810 - val_loss: 1.9776 - val_binary_accuracy: 0.8044\n",
            "Run:  5 / Accuracy at epoch  15  is:  0.8044000267982483 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.1730 - binary_accuracy: 0.9844 - val_loss: 2.3196 - val_binary_accuracy: 0.8158\n",
            "Run:  5 / Accuracy at epoch  16  is:  0.8158000111579895 \n",
            "\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.1912 - binary_accuracy: 0.9822 - val_loss: 3.6443 - val_binary_accuracy: 0.8038\n",
            "Run:  5 / Accuracy at epoch  17  is:  0.8037999868392944 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.1729 - binary_accuracy: 0.9904 - val_loss: 3.5421 - val_binary_accuracy: 0.8192\n",
            "Run:  5 / Accuracy at epoch  18  is:  0.8191999793052673 \n",
            "\n",
            "5000/5000 [==============================] - 808s 161ms/step - loss: 0.1457 - binary_accuracy: 0.9892 - val_loss: 2.9491 - val_binary_accuracy: 0.8254\n",
            "Run:  5 / Accuracy at epoch  19  is:  0.8253999948501587 \n",
            "\n",
            "Accuracies over epochs: [0.5590000152587891, 0.676800012588501, 0.7224000096321106, 0.7882000207901001, 0.819599986076355, 0.8335999846458435, 0.8465999960899353, 0.840399980545044, 0.8428000211715698, 0.7491999864578247, 0.7947999835014343, 0.7936000227928162, 0.8205999732017517, 0.8253999948501587, 0.8176000118255615, 0.8044000267982483, 0.8158000111579895, 0.8037999868392944, 0.8191999793052673, 0.8253999948501587] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Over all runs maximum accuracies are: [0.8335999846458435, 0.840399980545044, 0.8435999751091003, 0.843999981880188, 0.8465999960899353]\n",
            "The median is: 0.8435999751091003 \n",
            "\n",
            "\n",
            "\n",
            "BiLSTM Accuracy Score on Test set ->  ['0.8435999751091003 +/- 0.009999990463256836']\n",
            "\n",
            "\n",
            "* * * * EVALUATION USING LOW_STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 1533\n",
            "Vocabulary size is: 94013\n",
            "5000/5000 [==============================] - 810s 161ms/step - loss: 0.7013 - binary_accuracy: 0.5092 - val_loss: 0.6447 - val_binary_accuracy: 0.6318\n",
            "Run:  1 / Accuracy at epoch  0  is:  0.6317999958992004 \n",
            "\n",
            "5000/5000 [==============================] - 809s 161ms/step - loss: 0.6591 - binary_accuracy: 0.6578 - val_loss: 0.6235 - val_binary_accuracy: 0.7522\n",
            "Run:  1 / Accuracy at epoch  1  is:  0.7522000074386597 \n",
            "\n",
            "5000/5000 [==============================] - 819s 163ms/step - loss: 0.5932 - binary_accuracy: 0.7510 - val_loss: 0.5936 - val_binary_accuracy: 0.7834\n",
            "Run:  1 / Accuracy at epoch  2  is:  0.7833999991416931 \n",
            "\n",
            "5000/5000 [==============================] - 822s 164ms/step - loss: 0.5648 - binary_accuracy: 0.7736 - val_loss: 0.6079 - val_binary_accuracy: 0.7762\n",
            "Run:  1 / Accuracy at epoch  3  is:  0.776199996471405 \n",
            "\n",
            "5000/5000 [==============================] - 823s 164ms/step - loss: 0.5131 - binary_accuracy: 0.8116 - val_loss: 0.7848 - val_binary_accuracy: 0.8098\n",
            "Run:  1 / Accuracy at epoch  4  is:  0.8098000288009644 \n",
            "\n",
            "5000/5000 [==============================] - 823s 164ms/step - loss: 0.4409 - binary_accuracy: 0.8614 - val_loss: 0.7314 - val_binary_accuracy: 0.8094\n",
            "Run:  1 / Accuracy at epoch  5  is:  0.8094000220298767 \n",
            "\n",
            "5000/5000 [==============================] - 823s 164ms/step - loss: 0.3982 - binary_accuracy: 0.8988 - val_loss: 0.6740 - val_binary_accuracy: 0.8322\n",
            "Run:  1 / Accuracy at epoch  6  is:  0.8321999907493591 \n",
            "\n",
            "5000/5000 [==============================] - 823s 164ms/step - loss: 0.3330 - binary_accuracy: 0.9260 - val_loss: 0.6901 - val_binary_accuracy: 0.8448\n",
            "Run:  1 / Accuracy at epoch  7  is:  0.8447999954223633 \n",
            "\n",
            "5000/5000 [==============================] - 825s 164ms/step - loss: 0.3182 - binary_accuracy: 0.9396 - val_loss: 0.6799 - val_binary_accuracy: 0.8466\n",
            "Run:  1 / Accuracy at epoch  8  is:  0.8465999960899353 \n",
            "\n",
            "5000/5000 [==============================] - 818s 163ms/step - loss: 0.3187 - binary_accuracy: 0.9482 - val_loss: 1.1636 - val_binary_accuracy: 0.8206\n",
            "Run:  1 / Accuracy at epoch  9  is:  0.8205999732017517 \n",
            "\n",
            "5000/5000 [==============================] - 815s 162ms/step - loss: 0.3125 - binary_accuracy: 0.9550 - val_loss: 1.8729 - val_binary_accuracy: 0.7772\n",
            "Run:  1 / Accuracy at epoch  10  is:  0.7771999835968018 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.2856 - binary_accuracy: 0.9640 - val_loss: 2.2304 - val_binary_accuracy: 0.8096\n",
            "Run:  1 / Accuracy at epoch  11  is:  0.8095999956130981 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.2518 - binary_accuracy: 0.9770 - val_loss: 2.7906 - val_binary_accuracy: 0.7508\n",
            "Run:  1 / Accuracy at epoch  12  is:  0.7508000135421753 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.2239 - binary_accuracy: 0.9760 - val_loss: 1.8681 - val_binary_accuracy: 0.7858\n",
            "Run:  1 / Accuracy at epoch  13  is:  0.7857999801635742 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.2217 - binary_accuracy: 0.9800 - val_loss: 1.6095 - val_binary_accuracy: 0.8222\n",
            "Run:  1 / Accuracy at epoch  14  is:  0.8222000002861023 \n",
            "\n",
            "5000/5000 [==============================] - 814s 162ms/step - loss: 0.1978 - binary_accuracy: 0.9844 - val_loss: 3.3147 - val_binary_accuracy: 0.7590\n",
            "Run:  1 / Accuracy at epoch  15  is:  0.7590000033378601 \n",
            "\n",
            "5000/5000 [==============================] - 813s 162ms/step - loss: 0.1841 - binary_accuracy: 0.9864 - val_loss: 4.4408 - val_binary_accuracy: 0.7440\n",
            "Run:  1 / Accuracy at epoch  16  is:  0.7440000176429749 \n",
            "\n",
            "1922/5000 [==========>...................] - ETA: 5:43 - loss: 0.1440 - binary_accuracy: 0.9891"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(0)\n",
        "TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS=True\n",
        "\n",
        "# Reset model_results list.\n",
        "for key in prepro_functions_dict_comb:\n",
        "  model_results[key]=[]\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  runs_accuracy = []\n",
        "\n",
        "  print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
        "\n",
        "  # Preprocess training set to build a dictionary.\n",
        "  vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
        "\n",
        "  max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
        "  print(\"Vocabulary size is:\", max_features)\n",
        "\n",
        "  for run in range(1,(num_runs+1)):\n",
        "    epochs_accuracy=[]\n",
        "    model = tf.keras.Sequential([\n",
        "                                    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                                    vectorize_layer,\n",
        "                                    layers.Embedding(max_features + 1, embedding_dim),                     \n",
        "                                    layers.Dropout(0.8),\n",
        "\n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "                                    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
        "                                    tf.keras.layers.Dropout(0.5),\n",
        "                                    tf.keras.layers.Dense(1)                            \n",
        "    ])\n",
        "    model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "\n",
        "    for epoch in range (0,num_epochs_per_run):\n",
        "        history = model.fit(\n",
        "          train_ds,\n",
        "          validation_data = test_ds,\n",
        "          epochs=1,\n",
        "          shuffle=False,\n",
        "          # Comment the following line to do not save and download the model.\n",
        "          #callbacks=[callbacks]\n",
        "          )\n",
        "        accuracy = history.history['val_binary_accuracy']\n",
        "        print(\"Run: \",run,\"/ Accuracy at epoch \",epoch,\" is: \", accuracy[0],\"\\n\")\n",
        "        epochs_accuracy.append(accuracy[0])\n",
        "\n",
        "    print(\"Accuracies over epochs:\",epochs_accuracy,\"\\n\\n\")\n",
        "    runs_accuracy.append(max(epochs_accuracy))\n",
        "\n",
        "  runs_accuracy.sort()\n",
        "  print(\"\\n\\n Over all runs maximum accuracies are:\", runs_accuracy)\n",
        "  print(\"The median is:\",runs_accuracy[2],\"\\n\\n\\n\")\n",
        "  \n",
        "  if (runs_accuracy[2]-runs_accuracy[0])>(runs_accuracy[4]-runs_accuracy[2]):\n",
        "    max_range_from_median = runs_accuracy[2]-runs_accuracy[0]\n",
        "  else:\n",
        "    max_range_from_median = runs_accuracy[4]-runs_accuracy[2]\n",
        "  final_result = str(runs_accuracy[2])+\" +/- \"+ str(max_range_from_median)\n",
        "  model_results[key].append(final_result)\n",
        "  print(\"BiLSTM Accuracy Score on Test set -> \",model_results[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uMKAeuegNk1"
      },
      "source": [
        "## Now show compact results in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ79RMougNRA"
      },
      "outputs": [],
      "source": [
        "print(\" PREPRO FUNCTION    |  Test Accuracy   |\",end = '')\n",
        "\n",
        "print(\"\\n\")\n",
        "for prepro_func in prepro_functions_dict_comb:\n",
        "  #print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\n",
        "  result = format(round(model_results[prepro_func][0],4),'.4f')\n",
        "  print(f'{prepro_func:27}{ result :12}')\n",
        "  print(\"\\n\")\n",
        " "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RgfUp_5TWED7"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}